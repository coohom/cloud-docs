<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js rust">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>MINERVAS Help Document</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "rust";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="dsl.html"><strong aria-hidden="true">2.</strong> DSL Programing Guide</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="scene_processor.html"><strong aria-hidden="true">2.1.</strong> Scene Processor</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="room.html"><strong aria-hidden="true">2.1.1.</strong> Room</a></li><li class="chapter-item expanded "><a href="level.html"><strong aria-hidden="true">2.1.2.</strong> Level</a></li></ol></li><li class="chapter-item expanded "><a href="entity_processor.html"><strong aria-hidden="true">2.2.</strong> Entity Processor</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="dsl/camera.html"><strong aria-hidden="true">2.2.1.</strong> Camera</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="dsl/perspective_camera.html"><strong aria-hidden="true">2.2.1.1.</strong> PerspectiveCamera</a></li><li class="chapter-item expanded "><a href="dsl/orthographic_camera.html"><strong aria-hidden="true">2.2.1.2.</strong> OrthographicCamera</a></li><li class="chapter-item expanded "><a href="dsl/panoramic_camera.html"><strong aria-hidden="true">2.2.1.3.</strong> PanoramicCamera</a></li></ol></li><li class="chapter-item expanded "><a href="dsl/light.html"><strong aria-hidden="true">2.2.2.</strong> Light</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="dsl/point_light.html"><strong aria-hidden="true">2.2.2.1.</strong> PointLight</a></li><li class="chapter-item expanded "><a href="dsl/rectangle_light.html"><strong aria-hidden="true">2.2.2.2.</strong> RectangleLight</a></li><li class="chapter-item expanded "><a href="dsl/sun_light.html"><strong aria-hidden="true">2.2.2.3.</strong> Sunlight</a></li><li class="chapter-item expanded "><a href="dsl/iesspot_light.html"><strong aria-hidden="true">2.2.2.4.</strong> IESspotLight</a></li></ol></li><li class="chapter-item expanded "><a href="dsl/instance.html"><strong aria-hidden="true">2.2.3.</strong> Instance</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="dsl/material.html"><strong aria-hidden="true">2.2.3.1.</strong> Material</a></li><li class="chapter-item expanded "><a href="dsl/mesh.html"><strong aria-hidden="true">2.2.3.2.</strong> Model</a></li></ol></li><li class="chapter-item expanded "><a href="dsl/trajectory.html"><strong aria-hidden="true">2.2.4.</strong> Trajectory</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="dsl/random_trajectory.html"><strong aria-hidden="true">2.2.4.1.</strong> RandomTrajectory</a></li><li class="chapter-item expanded "><a href="dsl/converage_trajectory.html"><strong aria-hidden="true">2.2.4.2.</strong> CoverageTrajectory</a></li><li class="chapter-item expanded "><a href="dsl/defined_trajectory.html"><strong aria-hidden="true">2.2.4.3.</strong> DefinedTrajectory</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="render_processor.html"><strong aria-hidden="true">2.3.</strong> Render Processor</a></li><li class="chapter-item expanded "><a href="pixel_processor.html"><strong aria-hidden="true">2.4.</strong> Pixel Processor</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="dsl/pixel_process/output_selection.html"><strong aria-hidden="true">2.4.1.</strong> Output Selection</a></li><li class="chapter-item expanded "><a href="dsl/pixel_process/noise.html"><strong aria-hidden="true">2.4.2.</strong> Noise Simulation</a></li><li class="chapter-item expanded "><a href="dsl/pixel_process/distortion.html"><strong aria-hidden="true">2.4.3.</strong> Distortion Simulation</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="examples.html"><strong aria-hidden="true">3.</strong> Examples</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="examples/layout_estimation.html"><strong aria-hidden="true">3.1.</strong> Layout Estimation</a></li><li class="chapter-item expanded "><a href="examples/semantic_segmentation.html"><strong aria-hidden="true">3.2.</strong> Semantic Segmentation</a></li><li class="chapter-item expanded "><a href="examples/depth_estimation.html"><strong aria-hidden="true">3.3.</strong> Depth Estimation</a></li><li class="chapter-item expanded "><a href="examples/trajectory_sampling.html"><strong aria-hidden="true">3.4.</strong> SLAM</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">MINERVAS Help Document</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#introduction-to-minervas" id="introduction-to-minervas">Introduction to MINERVAS</a></h1>
<!-- MINERVAS is a scene cognitive training dataset solution for the interior scene agent industry. Users can use MINERVAS to generate large-scale low-cost scene recognition data sets based on Coohom's massive scene sets.

The core workflow of MINERVAS includes the following parts: -->
<!-- * `Scene Processor`: In this stage, user can filter and re-arrange the layout of 3D scene
* `Entity Processor`: User can modify scene content such as Light, Mesh, Material, as well as the camera. We also support to export the customized scene related structure information as JSON output -->
<!-- * `Render Processor`: Based on the processed 3D scene, use render engine to generate RGB and auxiliary channel images such as Depth, Normal, Semantic etc -->
<!-- * `Pixel Processor`: In this stage, user can add image processing algorithm to the previous stage output -->
<!-- Currently, we have deployed MINERVAS as an online system for public usage. Please check the [User Manual](./user_manual.md) for details. -->
<p>MINERVAS is a <em>Massive INterior EnviRonments VirtuAl Synthesis</em> system. It aims to facilitate various vision problems by providing a programmable imagery data synthesis platform.</p>
<!-- Here are some example outputs:
![Outputs](images/minervas_output_example.png) -->
<p>Based on the large-scale (more than 50 million) high-quality (professional artist designed) database of <a href="https://www.kujiale.com">Kujiale.com</a>, MINERVAS provides a way for all users to access and manipulate them for facilitating their data-driven task.</p>
<p><img src="images/Pipeline.png" alt="System Pipeline" /></p>
<p>The pipeline of MINERVAS includes the following parts:</p>
<ul>
<li><code>Scene Process Stage</code>: In this stage, users can filter scenes by their condition and re-arrange the layout of 3D scenes for domain randomization.</li>
<li><code>Entity Process Stage</code>: This Stage is designed for batch processing entities in the scene. Users can easily use entity-level samplers to randomize attributes of each entity, including furniture (e.g., CAD model, material, transformation), light (e.g., intensity, color), and camera (e.g., camera model, transformation). Modifying the attribute of each object manually is also supported.</li>
<li><code>Render Stage</code>: the system uses the generated scenes to generate 2D renderings with the photo-realistic rendering engine. </li>
<li><code>Pixel Process Stage</code>: In this stage, users can apply pixel-wise processing operations on the imagery data.</li>
</ul>
<p>Considering the flexibility and ease of use of the system, MINERVAS provides two ways of usage: a user-friendly GUI mode and a flexible programmable mode. In the programmable mode, the pipeline is fully controlled by the Domain-Specific Language (DSL). The DSL of MINERVAS is based on Python programming languages and contains multiple useful built-in functions as we will introduce in the next chapter. Moreover, as the diversity of the data is crucial for learning-based methods, MINERVAS also supports domain randomization both in GUI mode and programmable mode.</p>
<!-- To get started with the web interface of MINERVAS, please read the [*Getting Started* page](user_manual.md).  -->
<p>In this documentation, we provide a <a href="dsl.html">DSL programming guide</a> of MINERVAS along with some <a href="examples.html">examples</a> of vision tasks.</p>
<blockquote>
<p>For more information, please visit our <a href="https://coohom.github.io/MINERVAS/">project page</a>. For any problem in usage or any suggestion, please feel free to contact us <a href="mailto:minervas@qunhemail.com">minervas@qunhemail.com</a>.</p>
</blockquote>
<h1><a class="header" href="#dsl-programming-guide" id="dsl-programming-guide">DSL Programming Guide</a></h1>
<p>MINERVAS has a programmable dataset generation pipeline with Domain-Specific Language. As the main interface for users to cutomize the dataset generation for different task, we describe our DSL in this section.</p>
<!-- <span style="color:blue">*Comments:* Many formats in DSL description is not unified. </span> -->
<!-- toc -->
<h2><a class="header" href="#introduction-to-dsl-of-minervas" id="introduction-to-dsl-of-minervas">Introduction to DSL of MINERVAS</a></h2>
<p>The DSL (Domain-Specific Language) of MINERVAS is kind of internal DSL, it is based on Python programming languages. </p>
<p>Each DSL file submitted to MINERVAS system is a Python file ended with <code>.py</code>. </p>
<p>In the Python file, users need to:</p>
<ul>
<li>Declare one or more classes inheriting from corresponding built-in processor class.</li>
</ul>
<ul>
<li>Implement the cutomized operation in the <code>process()</code> function.</li>
</ul>
<!-- The essence of DSL writing is that the user customizes one or more subclasses in the py file. 
The subclasses:
* Three subclasses of the Processor class inherited from KsSDK
- Borrow the interface provided by the SDK and inherited attributes to implement custom functions in the `process()` function -->
<h2><a class="header" href="#processor-classes" id="processor-classes">Processor classes</a></h2>
<p>There are currently four processor classes in MINERVAS system, which reflect different stages of the dataset generation pipeline.</p>
<!-- <span style="color:blue">TODO: Need updates. </span> -->
<ol>
<li>SceneProcessor, provides custom filtering and modifying 3D scenes in scene-level.</li>
</ol>
<pre><code class="language-python">from ksecs.ECS.processors.scene_processor import SceneProcessor
class sceneDsl(SceneProcessor):
	def process(self, *args, **kwargs):
		pass
</code></pre>
<ol start="2">
<li>EntityProcessor, provides custom modifying of each attributes of objects in entity-level.</li>
</ol>
<pre><code class="language-python">from ksecs.ECS.processors.entity_processor import EntityProcessor
class entityDsl(EntityProcessor):
	def process(self, *args, **kwargs):
		pass
</code></pre>
<ol start="3">
<li>RenderProcessor, provides customization in the rendering process. </li>
</ol>
<pre><code class="language-python">from ksecs.ECS.processors.render_processor import RenderProcessor
class renderDsl(RenderProcessor):
	def process(self, *args, **kwargs):
		pass
</code></pre>
<ol start="4">
<li>PixelProcessor, provides customed post-processing of rendered image results</li>
</ol>
<pre><code class="language-python">from ksecs.ECS.processors.pixel_processor import PixelProcessor
class pixelDsl(PixelProcessor):
	def process(self, *args, **kwargs):
		pass
</code></pre>
<!-- 5. StructureProcessor, edit the output structured data
```python
from ksecs.ECS.processors.structure_processor import StructureProcessor
class structureDsl(StructureProcessor):
	def process(self, *args, **kwargs):
		pass
``` -->
<blockquote>
<p><strong>Tips</strong>: Don't forget to import the corresponding processor class from <code>ksces.ECS.processors</code> before use.</p>
</blockquote>
<!-- ## An attribute - shader -->
<h2><a class="header" href="#attribute-shader" id="attribute-shader">Attribute: shader</a></h2>
<p><code>shader</code> is a common attribute of all <code>Processor</code> classes. It is an instance of class <code>Shader</code>, which provides interface for accessing all 3D data assets and built-in function.</p>
<!-- Concretly, the class `Shader` has two attributes: `world` and `image_handler` which are instances of class `World` and class `ImageHandler`.

<span style="color:blue">*Comments:* `Shader` has any functions? </span> -->
<h3><a class="header" href="#world-class" id="world-class"><code>World</code> class</a></h3>
<!-- The user-defined class inherits the attribute shader, which connects to the underlying data structure of the SDK. -->
<!-- Shader is an instantiated object of class `Shader`, which has attributes world and image_handler -->
<!-- <span style="color:blue">*Comments:* More details (e.g. function list.) may be added for `World`, `Element` and `ImageHandler`.</span> -->
<!-- World is an instantiated object of class World, which is used to store the "database" of the input data of CC world, which is composed of various entities of Elment. -->
<p>The class <code>Shader</code> has an attribute: <code>world</code>.
<code>world</code> object is an instance of class <code>World</code>. The <code>World</code> class is an interface for a whole 3D scene in the database. It contains serveral elements:</p>
<table><thead><tr><th>Attribute</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td>instances</td><td>list of <code>Instance</code></td><td>All objects (furniture) in the scene</td></tr>
<tr><td>lights</td><td>list of <code>Light</code></td><td>All lights in the scene</td></tr>
<tr><td>rooms</td><td>list of <code>Room</code></td><td>All rooms in the scene (whole floorplan)</td></tr>
<tr><td>levels</td><td>list of <code>Level</code></td><td>Information for each level floors in the scene</td></tr>
<tr><td>trajectories</td><td>list of <code>Trajectory</code></td><td>All trajectories in the scene</td></tr>
<tr><td>cameras</td><td>list of <code>Camera</code></td><td>All exist cameras in the scene</td></tr>
</tbody></table>
<p>We will introduce each class in the following parts.</p>
<!-- #### Element
Currently supports six elements `Instance`, `Light`, `Room`, `Level`, `Trajectory`, and `Camera`.
Each element corresponds to a class, with its own attributes and methods -->
<table><thead><tr><th>Function</th><th>Description</th></tr></thead><tbody>
<tr><td>delete_entity((entity))</td><td>delete entity from the scene.</td></tr>
<tr><td>add_camera({attr_name}={attr_value})</td><td>create a new camera and add it to the scene</td></tr>
<tr><td>tune_brightness__all_lights(ratio)</td><td>adjust the brightness of all lights in the scene(ratio: brightness adjustment multiple)</td></tr>
<tr><td>tune_brightness__sunlight(ratio)</td><td>adjust the multiple of natural light and turn off other light sources. (ratio: brightness adjustment multiple)</td></tr>
<tr><td>replace_material(id, type, category)</td><td>domain randomization for materials. See <a href="./dsl/material.html">Material</a></td></tr>
<tr><td>replace_model(id)</td><td>domain randomization for models. See <a href="./dsl/mesh.html">Model</a></td></tr>
<tr><td>add_trajectory({attr_name}={attr_value})</td><td>create a new trajectory and add to the scene. See <a href="./dsl/trajectory.html">Trajectory</a></td></tr>
<tr><td>pick(**kwargs)</td><td>Save customized attributes as output. See example in <a href="./examples/layout_estimation.html">Layout Estimation</a>. This function is encouraged to be called in <code>StructureProcessor</code> for robustness.</td></tr>
</tbody></table>
<!-- |(TBD)|-| -->
<!-- ### `ImageHandler` class

`image_handler` object is an instance of class `ImageHandler`. This class contains severl image-related operations which we will introduce in [Noise Simulation](dsl/pixel_process/noise.md).

| Attribute | Type | Description    |
|---    |---  |---   |
| (TBD) | - | - |

| Function | Description    |
|---    |---   |
| save_files(cid, content, suffix, name) | - |
|(TBD)|-| -->
<h2><a class="header" href="#ecs-d" id="ecs-d">ECS-D</a></h2>
<p>Since our system supports customization to the scene, the user should easily access the scene with a suitable 3D scene representation. Thus, we employ the Entity Component System (ECS) architecture to represent and organize the 3D scene in our system. Additionally, to facilitate the randomness of scene synthesis, we integrate random distributions into the original ECS architecture, ie, attaching a distribution to depict each component. The newly proposed architecture is named as ECS-D, where D denotes distributions on components.</p>
<h1><a class="header" href="#scene-processor" id="scene-processor">Scene Processor</a></h1>
<!-- ## Description -->
<p>The scene process stage takes
the scene database as input and allows users
to filter desired scenes and modify the room layout of selected scenes from the database. 
Users need to implement a class inherite from <code>SceneProcessor</code> to control the scene process stage.</p>
<p>Each room is an instance of the <code>class Room</code>.
Based on these, users could filter scenes and modify the furniture layout of a room.</p>
<!-- ## Attributes
|Attributes |Type | Description    |
|---    |---    |--- |
| -|-|- |
## Function
|Function|Description|
|---|---|
|-|-| -->
<!-- |get_rooms()  |return the room list (list of `class Room`)| -->
<p>More description about <a href="./room.html">Room</a></p>
<h2><a class="header" href="#scene-filtering" id="scene-filtering">Scene filtering</a></h2>
<p>In <code>SceneProcessor</code>, user can customize their rules for filtering scenes from the database. For example, the room type, the number of rooms, the number of furniture in the room, etc.</p>
<!-- Filter a scene according to its room type. In this example, we filter scenes with kitchen. -->
<p>In the following, we provide a DSL code for generating scenes which have more than three rooms and has at least two bedrooms.</p>
<pre><code class="language-python">from ksecs.ECS.processors.scene_processor import SceneProcessor
class SceneFilterExample(SceneProcessor):
    def process(self):
        if len(self.shader.world.rooms) &lt;= 3:
            sys.exit(7)
        bedroom_count = 0
        for room in self.shader.world.rooms:
            if room.type == &quot;bedroom&quot;:
                bedroom_count += 1
        if bedroom_count &lt; 2:
            sys.exit(7)
</code></pre>
<p>Note that exit value 7 represents the exit is caused by unstatisfying scene.</p>
<p>For some simple filtering rules, we recommed users to use a more user-friendly <a href="https://www.kujiale.com/coohomcloud/kloudscene#/">GUI mode</a>.</p>
<!-- A [Web GUI](https://www.kujiale.com/coohomcloud/kloudscene#/) for scene filtering is more user-friendly. We recommend users to use filtering GUI instead of filtering in `SceneProcessor`. The `SceneProcessor` is optional. --><h1><a class="header" href="#room" id="room">Room</a></h1>
<!-- ## Description -->
<p>The room's information is encapsulated as <code>Room</code>.</p>
<!-- Users could sample furniture layout using function `sample()`. -->
<!-- Also, users could use functions, such as `get_polygon()`, to get room polygon for further processing. -->
<!-- <span style="color:blue">*Comments:* More info about `room` needs here.</span> -->
<h3><a class="header" href="#attributes" id="attributes">Attributes</a></h3>
<table><thead><tr><th>Attributes</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td>roomId</td><td>str</td><td>String for identifing the room.</td></tr>
<tr><td>position</td><td>list</td><td>2D coordinates of room center.</td></tr>
<tr><td>boundary</td><td>list</td><td>Boundary of room represented by corners in 2D coordinates.</td></tr>
<tr><td>name</td><td>str</td><td>The name of room.</td></tr>
<tr><td>area</td><td>str</td><td>The area of room.</td></tr>
</tbody></table>
<!-- ### Function

|Function   |Description    |
|---    |---    |
|-|-| -->
<!-- |get_height()   |return the height of the room  |
|get_floor_corners()    |return the floor corners of the room   |
|get_ceiling_corners()  |return the ceiling corners of the room | -->
<!-- |get_polygon()  |return the polygon of the rooom using `shapely`| -->
<!-- |height |height of the room (`int`)  |
|id     |id of the room (`str`)   |
|type   |type name of the room (`str`) | -->
<h2><a class="header" href="#domain-randomization---room-sampler" id="domain-randomization---room-sampler">Domain randomization - Room Sampler</a></h2>
<!-- ## Features -->
<!-- <span style="color:blue">*Comments:* `World/ccworld` and `Instance/CCInstance` should be consistent across the doc.</span> -->
<p>MINERVAS has a scene level sampler to generate novel furniture arrangements. Users can generate various reasonable furniture arrangements for domain randomization with this sampler.</p>
<p>Sampler code:</p>
<!-- For a given ccworld room, transform the space position of the CCInstance in the room to randomly generate a room layout. The new layout satisfies certain constraints and rules. -->
<!-- ## Usage

```python
class RandomizeLayout(SceneProcessor):
     def process(self):
         room = self.shader.world.get_room("room_id")
         room.randomize_layout(self.shader.world)
``` -->
<!-- ## Use constraints

The main limitation at present is that it can only take effect for scenarios where the label is KJL. There are two main reasons:

1. At present, only the instance with type=Asset supports modification of transform to render on the server side, and currently only the furniture instance of the scene with the KJL tag satisfies the conditions.
2. In the logic implemented by layout_sampler, a large number of Kujiale categories are used to make logical judgments (such as judging whether furniture should be posted on the wall according to the category). This point needs to be improved in the future, adding related attributes in CCInstance. -->
<!-- ## Example -->
<pre><code class="language-python">from ksecs.ECS.processors.scene_processor import SceneProcessor
class RoomSampler(SceneProcessor):
    def process(self):
        for room in self.shader.world.rooms:
            room.randomize_layout(self.shader.world)
</code></pre>
<p><img src="./examples_figs/layout_sampler.png" alt="room_sampler" /></p>
<h1><a class="header" href="#level" id="level">Level</a></h1>
<p>Level contains information of different floors in the scene. The height of each level is provided.</p>
<!-- (introduction here.) -->
<h3><a class="header" href="#attributes-1" id="attributes-1">Attributes</a></h3>
<table><thead><tr><th>Attributes</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td>id</td><td>str</td><td>The string for identifing the level.</td></tr>
<tr><td>height</td><td>float</td><td>The height of the level.</td></tr>
</tbody></table>
<!-- ### Function

|Function   |Description    |
|---    |---    |
|-|-| --><h1><a class="header" href="#entity-processor" id="entity-processor">Entity Processor</a></h1>
<!-- ## Description -->
<p>The entity process stage is designed for batch processing entities in the scene set. 
Users can implement an <code>EntityProcessor</code> to control the entity process stage.</p>
<p>Also, attributes (component) of each object (entity) can be manipulated in this processor, including:</p>
<ul>
<li><a href="dsl/camera.html">Camera</a></li>
<li><a href="dsl/light.html">Light</a></li>
<li><a href="dsl/trajectory.html">Trajectory</a></li>
<li><a href="dsl/instance.html">Transform</a></li>
<li><a href="dsl/material.html">Material</a></li>
<li><a href="dsl/mesh.html">Mesh</a></li>
</ul>
<!-- ## Attributes
|Attributes |Type | Description    |
|---    |---    |--- |
| -|-|- |
## Function
|Function|Description|
|---|---|
|-|-|  -->
<!-- ### Function

|Function   |Description    |
|---    |---    |
|delete(enitiy) |delete an entity in the scene|
|copy(entity)   |copy an entity in the scene|
|get_rooms()    |return the room list (list of `class Room`)|

Also, many components can be manipulated, including:

- [Camera Component](./camera.md)
- [Trajectory Component](./trajectory.md)
- [Light Component](./light.md)
- [Material Component](./material.md)
- [Mesh Component](./mesh.md)
- [Transform Component](./transform.md)
- [Other Component](./other.md)

## Example

Delete an entity in the scene according to its type.

```python
class EntityExample(EntityProcessor):
    def process(self):
        for entity, (furnitureComp, semanticComp) in self.world.get_components(FurnitureComponent, SemanticComponent):
            if semanticComp.get_category() == "Desk":
                self.delete(entity)
``` -->
<!-- ## Entity operation -->
<!-- <span style="color:blue">*Comments:* Need more details.</span> -->
<!-- ## Get Entity
TODO -->
<!-- ### Delete Entity
Function Description
* ```self.shader.world.delete_entity((entity))```: delete entity from the scene

example -->
<!-- ```python
class DeleteCameraDsl(EntityProcessor):
     def process(self):
         # loop all camera
         for camera in self.shader.world.cameras:
             # delete camera
             entityId = self.shader.world.delete_entity(camera)
``` -->
<!-- ### Create Entity -->
<h2><a class="header" href="#entity-filtering" id="entity-filtering">Entity filtering</a></h2>
<p>We filter out cameras in the room with few furniture in the following example DSL.</p>
<pre><code class="language-python">from ksecs.ECS.processors.entity_processor import EntityProcessor
from shapely.geometry import Point
class CameraFilterProcessor(EntityProcessor):
    def process(self):
        for room in self.shader.world.rooms:
            polygon = room.gen_polygon()
            furniture_count = 0
            for ins in self.shader.world.instances:
                if not ins.type == 'ASSET':
                    continue
                if polygon.contains(Point([ins.transform[i] for i in [3, 7, 11]])):
                    furniture_count += 1
            if furniture_count &lt; 5: # We only use room with more than or equal to 5 assets
                for camera in self.shader.world.cameras:
                    if polygon.contains(Point([camera.position[axis] for axis in &quot;xyz&quot;])):
                        self.shader.world.delete_entity(camera)
</code></pre>
<h1><a class="header" href="#camera" id="camera">Camera</a></h1>
<p>DSL supports the addition of three types of cameras, including:</p>
<ul>
<li><a href="dsl/../dsl/perspective_camera.html">Perspective Camera</a></li>
<li><a href="dsl/../dsl/orthographic_camera.html">Orthographic Camera</a></li>
<li><a href="dsl/../dsl/panoramic_camera.html">Panoramic Camera</a></li>
</ul>
<p>When adding a camera, you need to set the camera's parameters, including the common parameters of all types of cameras and the unique parameters of specific types of cameras.</p>
<h2><a class="header" href="#attributes-2" id="attributes-2">Attributes</a></h2>
<!-- ### General attributes -->
<table><thead><tr><th>Attribute</th><th>Type</th><th>Description</th><th>Default value</th><th>Required</th></tr></thead><tbody>
<tr><td>id</td><td>str</td><td>Camera ID, users need to add a prefix to ensure that the ID is unique</td><td>-</td><td>Yes</td></tr>
<tr><td>cameraType</td><td>str</td><td>Camera type, support PERSPECTIVE (perspective camera), ORTHO (orthogonal camera), PANORAMA (panoramic camera)</td><td>&quot;PERSPECTIVE&quot;</td><td></td></tr>
<tr><td>position</td><td>dict</td><td>Camera coordinates, the format is {'x':1,'y':2,'z':3}, the unit is mm</td><td>-</td><td>Yes</td></tr>
<tr><td>lookAt</td><td>dict</td><td>Target coordinates, the format is {'x':1,'y':2,'z':3}, the unit is mm</td><td>position+{'x':1,'y':0,'z': 0}</td><td></td></tr>
<tr><td>up</td><td>dict</td><td>Camera up direction, the format is {'x':1,'y':2,'z':3}, the unit is mm</td><td>{'x':0,'y':0,'z': 1}</td><td></td></tr>
<tr><td>imageWidth</td><td>int</td><td>Width of the image</td><td>-</td><td>Yes</td></tr>
<tr><td>imageHeight</td><td>int</td><td>The height of the image</td><td>-</td><td>Yes</td></tr>
<tr><td>near</td><td>float</td><td>Cut plane near</td><td>200</td><td></td></tr>
<tr><td>far</td><td>float</td><td>Cut plane far</td><td>2000000</td><td></td></tr>
</tbody></table>
<!-- |iso|float|Indicates the sensitivity of the camera|| -->
<!-- |fnumber|float|f value, indicating the aperture size|| -->
<!-- |shutterSpeed|float|The shutter speed of the camera, the unit is s^-1|| -->
<!-- ### Specific attributes
#### PERSPECTIVE camera
|Attribute|Description|Default value|Required|
|---|---|---|---|
|vfov|Vertical fov, that is, fov in OpenGL, angle value|none|yes|
|hfov|Horizontal fov, when it coexists with vfov, vfov shall prevail, the angle value|none||

#### ORTHO camera
|Attribute|Description|Default value|Required|
|---|---|---|---|
|orthoWidth|The width of the camera displayed in the model space, in millimeters|none|yes|
|orthoHeight|The height displayed by the camera in the model space, in millimeters|none|yes| -->
<!-- # Camera operations -->
<h2><a class="header" href="#function" id="function">Function</a></h2>
<table><thead><tr><th>Function</th><th>Description</th></tr></thead><tbody>
<tr><td>set_attr({attr_name}, *args, **kwargs)</td><td>Set the attributes of the camera, see the name of the camera attributes</td></tr>
</tbody></table>
<!-- |{attr_name}|Get the attributes of the camera, see the name of the camera attributes.| -->
<!-- toc -->
<h2><a class="header" href="#get-the-camera-and-its-attributes" id="get-the-camera-and-its-attributes">Get the camera and its attributes</a></h2>
<p>Function Description</p>
<ul>
<li><code>self.shader.world.cameras</code>: Get the camera list of the scene</li>
</ul>
<!-- * `self.shader.world.camera_ids`: get a list of camera ids of the scene -->
<ul>
<li><code>camera.{attr_name}</code>: Get the attributes of the camera, see the name of the camera attributes.</li>
</ul>
<p>example</p>
<pre><code class="language-python">class ReadCameraDsl(EntityProcessor):
    def process(self):
        # loop all camera
        for camera in self.shader.world.cameras:
            cameraType = camera.cameraType
</code></pre>
<h2><a class="header" href="#createadd-camera" id="createadd-camera">Create/Add Camera</a></h2>
<p>Function Description</p>
<ul>
<li><code>self.shader.world.add_camera({attr_name}={attr_value})</code>: create a new camera and add it to the scene</li>
<li><code>self.shader.world.create_camera({attr_name}={attr_value})</code>: create a camera</li>
</ul>
<p>example</p>
<pre><code class="language-python">class CreateCameraDsl(EntityProcessor):
    def process(self):
        # add camera
        camera = self.shader.world.add_camera(
        id=&quot;test_camera&quot;,
        cameraType=&quot;PERSPECTIVE&quot;,
        position={'x':0,'y':0,'z':1000},
        lookAt={'x':100,'y':0,'z':1000},
        imageWidth=1280,
        imageHeight=720,
        vfov=90
        hfov=105
        )
</code></pre>
<h2><a class="header" href="#modify-camera" id="modify-camera">Modify camera</a></h2>
<p>Function Description</p>
<ul>
<li><code>camera.set_attr({attr_name}, *args, **kwargs)</code><sup class="footnote-reference"><a href="#args description">1</a></sup>: modify the attributes of the camera
example</li>
</ul>
<pre><code class="language-python">class SetCameraDsl(EntityProcessor):
    def process(self):
        for camera in self.shader.world.cameras:
            # set camera attr
            camera.set_attr('position', x=100, y=0, z=1000)
            camera.set_attr('imageWidth', 1280)
            camera.set_attr('hfov', 90, &quot;degree&quot;)
</code></pre>
<div class="footnote-definition" id="args description"><sup class="footnote-definition-label">1</sup>
<p><code>*args</code> may have multiple input parameters, in addition to the value of hfov, vfov, you can also specify the unit as <code>&quot;degree&quot;</code>/<code>&quot;rad&quot;</code>; <code>**kwargs</code> is used as a dictionary Type of attributes, such as position, etc.</p>
</div>
<!-- ## Viewport selection

Our system also provide some pre-defined viewport for users.
<span style="color:blue">*Comments:* Any more view? </span>

### Top-down view
Usage:
```python
class TopView(EntityProcessor):
    def process(self):
        self.gen_topview(width, height)
``` -->
<h2><a class="header" href="#domain-randomization---camera-sampler" id="domain-randomization---camera-sampler">Domain randomization - Camera Sampler</a></h2>
<p>Randomize camera position and view direction for <code>PanoramicCamera</code>.</p>
<pre><code class="language-python">import numpy as np
class CameraRandomizer(EntityProcessor):
    def process(self):
        for camera in self.shader.world.cameras:
            random_vec = np.random.normal(0, 1, size=3)
            camera_pos = np.array(list(camera.position.values()))
            randomized_pos = camera_pos + random_vec * np.array([500.0, 500.0, 50.0])
            camera.set_attr('position', x=randomized_pos[0], y=randomized_pos[1], z=randomized_pos[2])
            camera.set_attr('lookAt', z=randomized_pos[2])
</code></pre>
<h1><a class="header" href="#perspectivecamera" id="perspectivecamera">PerspectiveCamera</a></h1>
<h2><a class="header" href="#attributes-3" id="attributes-3">Attributes</a></h2>
<!-- ### General attributes -->
<table><thead><tr><th>Attribute</th><th>Type</th><th>Description</th><th>Default value</th><th>Required</th></tr></thead><tbody>
<tr><td>id</td><td>str</td><td>Camera ID, users need to add a prefix to ensure that the ID is unique</td><td>-</td><td>Yes</td></tr>
<tr><td>position</td><td>dict</td><td>Camera coordinates, the format is {'x':1,'y':2,'z':3}, the unit is mm</td><td>-</td><td>Yes</td></tr>
<tr><td>lookAt</td><td>dict</td><td>Target coordinates, the format is {'x':1,'y':2,'z':3}, the unit is mm</td><td>position+{'x':1,'y':0,'z': 0}</td><td></td></tr>
<tr><td>up</td><td>dict</td><td>Camera up direction, the format is {'x':1,'y':2,'z':3}, the unit is mm</td><td>{'x':0,'y':0,'z': 1}</td><td></td></tr>
<tr><td>imageWidth</td><td>float</td><td>Width of the image</td><td>-</td><td>Yes</td></tr>
<tr><td>imageHeight</td><td>float</td><td>The height of the image</td><td>-</td><td>Yes</td></tr>
<tr><td>near</td><td>float</td><td>Cut plane near</td><td>200</td><td></td></tr>
<tr><td>far</td><td>float</td><td>Cut plane far</td><td>2000000</td><td></td></tr>
<tr><td>vfov</td><td>float</td><td>Vertical fov, fov in OpenGL, the angle value</td><td>-</td><td>yes</td></tr>
<tr><td>hfov</td><td>float</td><td>Horizontal fov, when it coexists with vfov, vfov shall prevail, the angle value</td><td>-</td><td></td></tr>
</tbody></table>
<!-- |cameraType|str|Camera type, support PERSPECTIVE (perspective camera), ORTHO (orthogonal camera), PANORAMA (panoramic camera)|"PERSPECTIVE"| -->
<!-- |iso||Indicates the sensitivity of the camera|| -->
<!-- |fnumber||f value, indicating the aperture size|| -->
<!-- |shutterSpeed||The shutter speed of the camera, the unit is s^-1|| --><h1><a class="header" href="#orthographiccamera" id="orthographiccamera">OrthographicCamera</a></h1>
<h2><a class="header" href="#attributes-4" id="attributes-4">Attributes</a></h2>
<!-- ### General attributes -->
<table><thead><tr><th>Attribute</th><th>Type</th><th>Description</th><th>Default value</th><th>Required</th></tr></thead><tbody>
<tr><td>id</td><td>str</td><td>Camera ID, users need to add a prefix to ensure that the ID is unique</td><td>-</td><td>Yes</td></tr>
<tr><td>position</td><td>dict</td><td>Camera coordinates, the format is {'x':1,'y':2,'z':3}, the unit is mm</td><td>-</td><td>Yes</td></tr>
<tr><td>lookAt</td><td>dict</td><td>Target coordinates, the format is {'x':1,'y':2,'z':3}, the unit is mm</td><td>position+{'x':1,'y':0,'z': 0}</td><td></td></tr>
<tr><td>up</td><td>dict</td><td>Camera up direction, the format is {'x':1,'y':2,'z':3}, the unit is mm</td><td>{'x':0,'y':0,'z': 1}</td><td></td></tr>
<tr><td>imageWidth</td><td>float</td><td>Width of the image</td><td>-</td><td>Yes</td></tr>
<tr><td>imageHeight</td><td>float</td><td>The height of the image</td><td>-</td><td>Yes</td></tr>
<tr><td>near</td><td>float</td><td>Cut plane near</td><td>200</td><td></td></tr>
<tr><td>far</td><td>float</td><td>Cut plane far</td><td>2000000</td><td></td></tr>
<tr><td>orthoWidth</td><td>float</td><td>The width of the camera displayed in the model space, in millimeters</td><td>-</td><td>yes</td></tr>
<tr><td>orthoHeight</td><td>float</td><td>The height displayed by the camera in the model space, in millimeters</td><td>-</td><td>yes</td></tr>
</tbody></table>
<!-- |cameraType|str|Camera type, support PERSPECTIVE (perspective camera), ORTHO (orthogonal camera), PANORAMA (panoramic camera)|"PERSPECTIVE"| -->
<!-- |iso|float|Indicates the sensitivity of the camera|| -->
<!-- |fnumber||f value, indicating the aperture size|| -->
<!-- |shutterSpeed||The shutter speed of the camera, the unit is s^-1|| --><h1><a class="header" href="#panoramiccamera" id="panoramiccamera">PanoramicCamera</a></h1>
<!-- <span style="color:blue">*Comments:* Any new attributes?.</span> -->
<h2><a class="header" href="#attributes-5" id="attributes-5">Attributes</a></h2>
<!-- ### General attributes -->
<table><thead><tr><th>Attribute</th><th>Type</th><th>Description</th><th>Default value</th><th>Required</th></tr></thead><tbody>
<tr><td>id</td><td>str</td><td>Camera ID, users need to add a prefix to ensure that the ID is unique</td><td>-</td><td>Yes</td></tr>
<tr><td>position</td><td>dict</td><td>Camera coordinates, the format is {'x':1,'y':2,'z':3}, the unit is mm</td><td>-</td><td>Yes</td></tr>
<tr><td>lookAt</td><td>dict</td><td>Target coordinates, the format is {'x':1,'y':2,'z':3}, the unit is mm</td><td>position+{'x':1,'y':0,'z': 0}</td><td></td></tr>
<tr><td>up</td><td>dict</td><td>Camera up direction, the format is {'x':1,'y':2,'z':3}, the unit is mm</td><td>{'x':0,'y':0,'z': 1}</td><td></td></tr>
<tr><td>imageWidth</td><td>float</td><td>Width of the image</td><td>-</td><td>Yes</td></tr>
<tr><td>imageHeight</td><td>float</td><td>The height of the image</td><td>-</td><td>Yes</td></tr>
<tr><td>near</td><td>float</td><td>Cut plane near</td><td>200</td><td></td></tr>
<tr><td>far</td><td>float</td><td>Cut plane far</td><td>2000000</td><td></td></tr>
</tbody></table>
<!-- |iso||Indicates the sensitivity of the camera|| -->
<!-- |fnumber||f value, indicating the aperture size|| -->
<!-- |shutterSpeed||The shutter speed of the camera, the unit is s^-1|| -->
<!-- |(TBD)|-|-|-| -->
<!-- |cameraType|str|Camera type, support PERSPECTIVE (perspective camera), ORTHO (orthogonal camera), PANORAMA (panoramic camera)|"PERSPECTIVE"| --><h1><a class="header" href="#light" id="light">Light</a></h1>
<p>DSL supports four types of lights. There are PointLight, RectangleLight, Sunlight, IESspotLight.</p>
<!-- Each type of light has its own parameters which we list in the following. -->
<!-- The SDK provides the adjustment of lighting effects in the form of interfaces, so the attributes that can be directly adjusted are only the following list -->
<!-- <span style="color:blue">*Comments:* **emission (intensity, color temperature)** attribute is missing?</span>. -->
<h2><a class="header" href="#attributes-6" id="attributes-6">Attributes</a></h2>
<!-- ### General attributes -->
<table><thead><tr><th>Attribute</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td>lightType</td><td>str</td><td>PointLight, RectangleLight, SunLight, IESspotLight</td></tr>
<tr><td>energy</td><td>float</td><td>The intensity of light.</td></tr>
<tr><td>color</td><td>dict</td><td>The color of light. The format is {x&quot;: 3.4734852, &quot;y&quot;: 6.955175, &quot;z&quot;: 6.3826585}. It is the normalized rgb multipled by the energy.</td></tr>
</tbody></table>
<!-- ### Specific attributes -->
<!-- 
#### RectangleLight
The size of rectangel light is determined by U and V vectors. The normal direction is a unit vector with the direction of cross prodcut of U and V vector.

|Attribute|Description|
|---|---|
|directionU|The format is {"x": 0.0, "y": -291.0, "z": 0.0}|
|normalDirection|The format is {"x": -0.0, "y": -0.0, "z": -1.0}|
|position|The format is {"x": 191.20065,"y": 9078.513,"z": 69.999985}, the unit is mm| -->
<!-- #### PointLight
Point light radiates illumination into all directions uniformly.

|Attribute|Description|
|---|---|
|position|The format is {"x": 191.20065,"y": 9078.513,"z": 69.999985}| -->
<!-- #### SunLight
Sun light is direction light. It radiates a specified power per unit area along a fixed direction.

|Attribute|Description|
|---|---|
|direction|The format is {"x": -0.57735026, "y": 0.57735026, "z": 0.57735026}| -->
<!-- #### IESspotLight -->
<!-- IESspotlight is a spotlight with measured IES profile, which can provide realistic lighting effect. -->
<!-- <span style="color:blue">*Comments:* IES profile can be selected? Or it always uses a default IES profile. More discussion about IES may be added here.</span>. -->
<!-- |Attribute|Description| -->
<!-- |---|---| -->
<!-- |direction|The format is {"x": -0.57735026, "y": 0.57735026, "z": 0.57735026}| -->
<!-- |position|The format is {"x": 191.20065,"y": 9078.513,"z": 69.999985}, the unit is mm| -->
<h3><a class="header" href="#access-the-attributes-of-light" id="access-the-attributes-of-light">Access the attributes of light</a></h3>
<!-- Function Description -->
<!-- * ```self.shader.world.lights```: Get the light list of the scene -->
<!-- * ```light.{attr_name}```: Get the attributes of the light. For the name of the light attribute, see: [Light](../dsl/light.md) -->
<p>example DSL:</p>
<pre><code class="language-python">class ReadLightDsl(EntityProcessor):
    def process(self):
        # loop all lights
        for light in self.shader.world.lights:
            position = light.position
</code></pre>
<!-- # Light operation -->
<h2><a class="header" href="#function-1" id="function-1">Function</a></h2>
<table><thead><tr><th>Function</th><th>Description</th></tr></thead><tbody>
<tr><td>set_attr({attr_name}, **kwargs)</td><td>modify light attributes. <code>**kwargs</code> is used for attributes of dictionary type, such as position, etc.</td></tr>
<tr><td>_tune_temp(delta)</td><td>Random adjust color temperature. (delta: weight for tuned color temperature) (1 - delta) * orig + delta * tuned</td></tr>
<tr><td>tune_random(ratio)</td><td>Random light intensity. 50% probability attenuates according to ratio, 50% probability is uniformly sampled from the interval [0.1, 0.3] to get the attenuation coefficient.</td></tr>
<tr><td>tune_intensity(ratio)</td><td>Set brightness attenutation. (ratio: brightness adjustment multiple)</td></tr>
</tbody></table>
<!-- toc -->
<!-- <span style="color:blue">*Comments:* Any function for creating a new light?</span>. -->
<h3><a class="header" href="#modify-the-lighting-attributes-directly" id="modify-the-lighting-attributes-directly">Modify the lighting attributes directly</a></h3>
<!-- Function Description
`light.set_attr({attr_name}, **kwargs)`[^args description]: modify light attributes -->
<p>example DSL:</p>
<pre><code class="language-python">class SetLightDsl(EntityProcessor):
    def process(self):
        for light in self.shader.world.lights:
            # set light attr
            light.set_attr('position', x=100, y=0, z=1000)
</code></pre>
<!-- [^args description]: `**kwargs` is used for attributes of dictionary type, such as position, etc. -->
<!-- ## Illumination adjustment interface -->
<!-- ### Single light adjustment -->
<!-- #### tune_temp -->
<!-- -Function: adjust color temperature -->
<!-- -Entry: delta -->
<!-- #### tune_random -->
<!-- 
-Function: Random light, 50% probability attenuates according to ratio, 50% probability is uniformly sampled from the interval [0.1, 0.3] to get the attenuation coefficient.
-Entry: ratio, see above for usage -->
<!-- #### tune_intensity -->
<!-- -Function: adjust brightness -->
<!-- -Input parameters: ratio, brightness adjustment multiple -->
<h3><a class="header" href="#the-overall-light-intensity-adjustment-of-the-scene" id="the-overall-light-intensity-adjustment-of-the-scene">The overall light intensity adjustment of the scene</a></h3>
<p>There are two built-in function of <code>EntityProcessor</code> which can tune intensity of all lights.</p>
<table><thead><tr><th>Function</th><th>Description</th></tr></thead><tbody>
<tr><td>tune_brightness__all_lights(ratio)</td><td>adjust the brightness of all lights in the scene(ratio: brightness adjustment multiple)</td></tr>
<tr><td>tune_brightness__sunlight(ratio)</td><td>adjust the multiple of natural light and turn off other light sources. (ratio: brightness adjustment multiple)</td></tr>
</tbody></table>
<!-- #### tune_brightness__all_lights -->
<!-- -Function: adjust the brightness of all lights in the scene -->
<!-- -Input parameters: ratio, brightness adjustment multiple -->
<!-- #### tune_brightness__sunlight -->
<!-- -Function: adjust the multiple of natural light and turn off other light sources -->
<!-- -Input parameters: ratio, brightness adjustment multiple -->
<p>Example of use:</p>
<pre><code class="language-python">class TuneLights(EntityProcessor):
    def process(self, *args, **kwargs):
        self.tune_brightness__all_lights(0.8)
</code></pre>
<h2><a class="header" href="#domain-randomization---light-sampler" id="domain-randomization---light-sampler">Domain randomization - Light Sampler</a></h2>
<p>Example DSL:</p>
<!-- ## Example -->
<!-- <span style="color:blue">*Comments:* `tune_temp` is not consistent with API above. Use color temperature or delta? </span>. -->
<!-- ```python
class LightSampler(EntityProcessor):
    def process(self):
        for light in self.shader.world.lights:
            # K值，可以根据K与rgb的map来设置，行业标准，越小越暖，越大越冷
            light.tune_temp(5000)
``` -->
<!-- Example of use: -->
<pre><code class="language-python">class LightsSampler(EntityProcessor):
    def process(self, *args, **kwargs):
        for light in self.shader.world.lights:
            # Adjust the color temperature randomly
            light._tune_temp(1)
            # Adjust the light intensity randomly
            light.tune_random(0.5)
            # Use lower light intensity
            light.tune_intensity(0.8)
</code></pre>
<p><img src="dsl/./../examples_figs/light_sampler.png" alt="light_sampler" /></p>
<h1><a class="header" href="#pointlight" id="pointlight">PointLight</a></h1>
<p>Point light radiates illumination into all directions uniformly from a point.</p>
<table><thead><tr><th>Attribute</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td>position</td><td>dict</td><td>The format is {&quot;x&quot;: 191.20065,&quot;y&quot;: 9078.513,&quot;z&quot;: 69.999985}</td></tr>
</tbody></table>
<h1><a class="header" href="#rectanglelight" id="rectanglelight">RectangleLight</a></h1>
<p>Rectangle light is a light emitted from the rectangle shape object.</p>
<p>The size of rectangel light is determined by U and V vectors. The normal direction is a unit vector with the direction of cross prodcut of U and V vector.</p>
<table><thead><tr><th>Attribute</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td>directionU</td><td>dict</td><td>The format is {&quot;x&quot;: 0.0, &quot;y&quot;: -291.0, &quot;z&quot;: 0.0}</td></tr>
<tr><td>normalDirection</td><td>dict</td><td>The format is {&quot;x&quot;: -0.0, &quot;y&quot;: -0.0, &quot;z&quot;: -1.0}</td></tr>
<tr><td>position</td><td>dict</td><td>The format is {&quot;x&quot;: 191.20065,&quot;y&quot;: 9078.513,&quot;z&quot;: 69.999985}, the unit is mm</td></tr>
</tbody></table>
<h1><a class="header" href="#sunlight" id="sunlight">Sunlight</a></h1>
<p>Sun light is the directional light. It radiates a specified power per unit area along a fixed direction.</p>
<table><thead><tr><th>Attribute</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td>direction</td><td>dict</td><td>The format is {&quot;x&quot;: -0.57735026, &quot;y&quot;: 0.57735026, &quot;z&quot;: 0.57735026}</td></tr>
</tbody></table>
<h1><a class="header" href="#iesspotlight" id="iesspotlight">IESspotLight</a></h1>
<p>IESspotlight is a spotlight with measured IES profile, which can provide realistic lighting effect.</p>
<!-- <span style="color:blue">*Comments:* IES profile can be selected? Or it always uses a default IES profile. More discussion about IES may be added here.</span>. -->
<table><thead><tr><th>Attribute</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td>direction</td><td>dict</td><td>The format is {&quot;x&quot;: -0.57735026, &quot;y&quot;: 0.57735026, &quot;z&quot;: 0.57735026}</td></tr>
<tr><td>position</td><td>dict</td><td>The format is {&quot;x&quot;: 191.20065,&quot;y&quot;: 9078.513,&quot;z&quot;: 69.999985}, the unit is mm</td></tr>
</tbody></table>
<h1><a class="header" href="#instance" id="instance">Instance</a></h1>
<!-- <span style="color:blue">*Comments:* Add more introduction of `Instance`</span>. -->
<p>Each object in the scene is an <code>Instance</code>. User can add/delete some instance, or changing the transformation/scale for each instance.</p>
<h2><a class="header" href="#attributes-7" id="attributes-7">Attributes</a></h2>
<table><thead><tr><th>Attribute</th><th>Type</th><th>Description</th><th>Default value</th><th>Required</th></tr></thead><tbody>
<tr><td>id</td><td>str</td><td>Instance ID, users need to add their own prefix to ensure that the ID is unique</td><td>-</td><td>Yes</td></tr>
<tr><td>label</td><td>int</td><td>The label of the instance, indicating the category to which the instance belongs</td><td>-</td><td>Yes</td></tr>
<tr><td>transform</td><td>list</td><td>The transformation matrix of the instance, which is a list type of a 4 x 4 matrix transformed according to <strong>row first</strong></td><td>-</td><td>Yes</td></tr>
<tr><td>type</td><td>str</td><td>Possible values are MESH | ASSET| COMPOSITE</td><td>-</td><td>Yes</td></tr>
</tbody></table>
<!-- |size|list|Properties only available when type is ASSET<span style="color:blue">*Comments:* more description here.</span>.||| -->
<h2><a class="header" href="#function-2" id="function-2">Function</a></h2>
<table><thead><tr><th>Function</th><th>Description</th></tr></thead><tbody>
<tr><td>set_attr({attr_name}, *args, **kwargs)</td><td>Set the attributes of the instance, see the name of the instance attributes</td></tr>
<tr><td>set_rotation(rotation_list)</td><td>Set rotation of instance. rotation_list: <code>list</code>. 3 x 3 matrix converted according to row first list.</td></tr>
<tr><td>set_scale(scale_list)</td><td>Set scale of instance. scale_list: <code>list</code>. a list of scaling factors</td></tr>
<tr><td>set_position(position_list)</td><td>Set position of instance. position_list: <code>list</code>. a list of position coordinates.</td></tr>
</tbody></table>
<!-- <span style="color:blue">*Comments:* `set_attr` not supported? </span>. -->
<h2><a class="header" href="#get-the-instance-and-its-attributes" id="get-the-instance-and-its-attributes">Get the instance and its attributes</a></h2>
<!-- Function Description
* ```self.shader.world.instances```: Get a list of instances of the scene
* ```instance.{attr_name}```: Get the attributes of the instance, see the name of the instance attribute: [Instance](../dsl/instance.md) -->
<p>Example</p>
<pre><code class="language-python">class ReadInstanceDsl(EntityProcessor):
    def process(self):
        # loop all instances
        for instance in self.shader.world.instances:
            label = instance.label
</code></pre>
<h2><a class="header" href="#add-instance" id="add-instance">Add instance</a></h2>
<p>Function Description</p>
<p><code>self.shader.world.add_instance({attr_name}={attr_value})</code>: Create a new instance and add it to the scene</p>
<p>Example:</p>
<pre><code class="language-python">class AddInstance(EntityProcessor):
    def process(self, *args, **kwargs):
        ins = self.shader.world.add_instance(
            id=&quot;test&quot;, label=1107, path=&quot;meshId&quot;, type=&quot;ASSET&quot;,
            transform=[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]
        )
</code></pre>
<!-- <span style="color:blue">*Comments:* Delete `Instance`. Unified for entities.</span>. -->
<h2><a class="header" href="#modify-instance-properties" id="modify-instance-properties">Modify instance properties</a></h2>
<h3><a class="header" href="#modify-the-instance-transformation-matrix" id="modify-the-instance-transformation-matrix">Modify the instance transformation matrix</a></h3>
<p>When modifying this attribute, it is achieved by rotating, panning and zooming, and one or more of the following operations can be performed</p>
<!-- #### Rotation example -->
<!-- `instance.set_rotation(rotation_list)` -->
<!-- *rotation_list: 3 x 3 matrix converted according to **row first** list* -->
<!-- #### Scaling example -->
<!-- `instance.set_scale(scale_list)` -->
<!-- *scale_list: a list of scaling factors* -->
<!-- #### Translation example -->
<!-- `instance.set_position(position_list)` -->
<!-- *position_list: a list of position coordinates* -->
<p>example:</p>
<pre><code class="language-python">class SetInstance(EntityProcessor):
    def process(self, *args, **kwargs):
        for ins in self.shader.world.instances():
            ins.set_position(
                [0, 0, 50]
            ).set_rotation(
                [1, 0, 0, 0, 1, 0, 0, 0, 1]
            ).set_scale([1, 1, 1])

</code></pre>
<h1><a class="header" href="#material" id="material">Material</a></h1>
<p>Material is an important component for every object in the scene. Our DSL also supports sampling new materials for each object for domain randomization.</p>
<p>Since the material is the core asset of the database, we only explore its index in the database and do not allow users to access the raw data.</p>
<h2><a class="header" href="#domain-randomization---material-sampler" id="domain-randomization---material-sampler">Domain randomization - Material Sampler</a></h2>
<!-- ## Material Replacement API -->
<p>MINERVAS provides <code>replace_material</code> method, which will provides following functionality:</p>
<ol>
<li><code>REPLACE_ALL</code>: Given a <code>Model</code> to randomly replace the material corresponding to each part (each part is sampled separately from the preset material library).</li>
<li><code>REPLACE_BY_CATEGORY</code>: Given a <code>Model</code> and the desired material category, and randomly replace the material of each part to the material of the corresponding material category. (Each part is sampled separately from the desired category of the preset material library)</li>
<li><code>REPLACE_TO_GIVEN_LIST</code>: Given a <code>Model</code> and the specified material id list, and randomly replace the material of each part to the material in the list.</li>
</ol>
<h3><a class="header" href="#function-parameters" id="function-parameters">Function parameters</a></h3>
<table><thead><tr><th align="left">First name</th><th align="left">Required or not</th><th align="left">Type</th><th align="left">Description</th></tr></thead><tbody>
<tr><td align="left">id</td><td align="left">Yes</td><td align="left">String</td><td align="left">Identifies the <code>Model</code> to be modified</td></tr>
<tr><td align="left">type</td><td align="left">Yes</td><td align="left"><code>REPLACE_ALL</code> | <code>REPLACE_BY_CATEGORY</code> | <code>REPLACE_TO_GIVEN_LIST</code></td><td align="left">replacement type</td></tr>
<tr><td align="left">category</td><td align="left">Required when type=<code>REPLACE_BY_CATEGORY</code></td><td align="left">String</td><td align="left">Category name to be replaced. Candidate material types currently: WOOD(0L),METAL(1),STONE(2).</td></tr>
<tr><td align="left">ids</td><td align="left">Required when type=<code>REPLACE_TO_GIVEN_LIST</code></td><td align="left">List of String</td><td align="left">ID list to be replaced</td></tr>
</tbody></table>
<!-- Usage:

```python
class ReplaceMaterial(EntityProcessor):
    def process(self, *args, **kwargs):
        for instance in self.shader.world.instances:
            self.shader.world.replace_material(
                id=instance.id,
                type='REPLACE_BY_CATEGORY',
                category='METAL'
            )
``` -->
<h3><a class="header" href="#example" id="example">Example</a></h3>
<!-- <span style="color:blue">*Comments:* TODO(@xuanfeng) Add information about material list api</span>. -->
<!-- <span style="color:blue">*Comments:* Code needs revision</span>. -->
<pre><code class="language-python">class MaterialSampler(EntityProcessor):
    def process(self):
        for instance in self.shader.world.instances:
            # floor category_id: 1227
            # sofa category_id: 1068 
            # carpet category_id: 1080
            if instance.label in [1227, 1068, 1080]:
                self.shader.world.replace_material(
                    id=instance.id,
                    type='REPLACE_ALL',
                )
</code></pre>
<p><img src="dsl/../examples_figs/material_sampler.png" alt="material_sampler" /></p>
<h1><a class="header" href="#model" id="model">Model</a></h1>
<p>In our system, the CAD model of each object in the scene can be easily replaced by user. 
The category of object remains the same for reasonable result.
Since the mesh is the core asset of the database, we only explore its index in the database and do not allow users to access the raw data.</p>
<!-- ## Randomly replace the model -->
<h2><a class="header" href="#domain-randomization---model-sampler" id="domain-randomization---model-sampler">Domain randomization - Model sampler</a></h2>
<!-- **command_type value: model_replace** -->
<!-- Requirements and background: -->
<!-- <span style="color:blue">*Comments:* CCInstance? Unify the name across the doc.</span>. -->
<p>MINERVAS provides <code>replace_model</code> method, which will provides following functionality:</p>
<p>The input the <code>id</code> of an <code>Instance</code> object and randomly replace the model of this object with a new model with the same semantic.</p>
<!-- Random replacement is valid for a given CCInstance (type=Asset only takes effect), other types of CCInstance may be ignored or an error may be reported. Then there are currently the following constraints (this part of the constraints can be gradually released with subsequent function development):

1. The size of the furniture before and after the replacement is kept the same (aligned by the scale parameter)
2. A furniture library needs to be preset, and only the model in the preset furniture library will be replaced. (This logic will be used as a bottom-up logic to ensure the robustness of the service)
   1. Models whose categories are not in the preset furniture library cannot be replaced. -->
<h3><a class="header" href="#function-parameters-1" id="function-parameters-1">Function parameters</a></h3>
<table><thead><tr><th align="left">First name</th><th align="left">Required or not</th><th align="left">Type</th><th align="left">Description</th></tr></thead><tbody>
<tr><td align="left">id</td><td align="left">Yes</td><td align="left">String</td><td align="left">Identifies the instance to be replaced</td></tr>
</tbody></table>
<!-- example:
```python
class ReplaceModel(EntityProcessor):
    def process(self, *args, **kwargs):
        for instance in self.shader.world.instances:
            self.shader.world.replace_model(
                id=instance.id
            )
``` -->
<h3><a class="header" href="#example-1" id="example-1">Example</a></h3>
<p>Now, we show a DSL code which randomly replace the model of sofa and table.</p>
<pre><code class="language-python">class MeshSampler(EntityProcessor):
    def process(self):
        for instance in self.shader.world.instances:
            # table category_id: 1032
            # sofa category_id: 1068 
            if instance.type == 'ASSET' and instance.label in [1032, 1068]:
                self.shader.world.replace_model(id=instance.id)
</code></pre>
<p><img src="dsl/../examples_figs/mesh_sampler.png" alt="mesh_sampler" /></p>
<h1><a class="header" href="#trajectory" id="trajectory">Trajectory</a></h1>
<p>DSL supports adding trajectory to the entity. Trajectory is an important component especially for robotic related tasks. In the MINERVAS system, users could sample a random trajectory or add a handcrafted trajectory to the camera.</p>
<p>There are three types of trajectories:</p>
<ul>
<li><a href="dsl/../dsl/random_trajectory.html">Random Trajectory</a></li>
<li><a href="dsl/../dsl/converage_trajectory.html">Coverage Trajectory</a></li>
<li><a href="dsl/../dsl/defined_trajectory.html">Defined Trajectory</a></li>
</ul>
<h2><a class="header" href="#attributes-8" id="attributes-8">Attributes</a></h2>
<!-- ## Trajectory types
Two type of trajectory are supported in the DSL.
1. Random trajectory. It can also be classified by the shape of trajectory.
* Bow-shape trajectory
* Pure random trajectory
2. Customized trajectory. User can generate the customize trajectory by tapping the key frame in the scene. -->
<!-- <span style="color:blue">*Comments:* Default values are missing in the following forms.</span>. -->
<!-- ### General attributes -->
<table><thead><tr><th>Attribute</th><th>Type</th><th>Description</th><th>Default value</th><th>Required</th></tr></thead><tbody>
<tr><td>type</td><td>str</td><td>RANDOM (random trajectory); COVERAGE (bow-shape trajectory); DEFINED (user customized trajectory, usually by tapping the key frame in the scene).</td><td>-</td><td>Yes</td></tr>
<tr><td>pitch</td><td>float</td><td>The angle of pitch</td><td>-</td><td>Yes</td></tr>
<tr><td>height</td><td>float</td><td>The height of camera. The unit is mm.</td><td>-</td><td>Yes</td></tr>
</tbody></table>
<!-- ### General attributes for RANDOM and COVERAGE type trajectory
|Attribute|Description|
|---|---|
|initCamera|Initialize camera. Input arguments are the same as [Camera](dsl/camera.md). |
|fps| Frames per second |
|speed| trajectory speed (the unit is mm/s) |
|colisionPadding| The radius of collision detection |

#### Specific attributes for RANDOM type trajectory
|Attribute|Description|
|---|---|
|time|duration of time (the unit is s)|

#### Specific attributes for COVERAGE type trajectory
|Attribute|Description|
|---|---|
|boundary|Restriction range of trajectory| -->
<!-- ### Specific attributes for DEFINED type trajectory -->
<!-- <span style="color:blue">*Comments:* This parameter list needs revision (e.g., description of keyPoints is incorrect).</span>.  -->
<!-- |Attribute|Description|Default value|Required|Remark|
|---|---|---|---|---|
|imageWidth|The width of rendered image||Yes||
|imageHeight|The height of rendered image||Yes||
|keyPoints| (Key points in image space. pixel indicies: [[[[x1, y1], [x2, y2], ...]]]) | | Yes | Three dimensional list. 1. list of keypoints set 2. list of pixel coordinates. 3. pixel coordinates|
|fps| Frames per second | | No | Required if `frameCount` is not specified |
|speed| trajectory speed (the unit is mm/s) | | No | Required if `frameCount` is not specified|
|speedMode| Mode for randomized speed, 0: initial randomization 1: procedual randomization | | No | Required if `speed` is specified |
|frameCount| Total frame count | | No | Required if `fps` is not specified |
|pitchMode| Mode of pitch randomization, 0: initial randomization, 1: procedual randomization | | Yes ||
|hfow| Horizontal field of view (the unit is degree) | | No | Required if camera type is default or 'PERSPECTIVE' |
|vfow| Vertical field of view (the unit is degree) | | No | Required if camera type is default or 'PERSPECTIVE' |
|orthoWidth| horizontal field of view (the unit is mm) | | No | Required if camera type is default or 'PERSPECTIVE' |
|orthoHeight| vertical field of view (the unit is mm) | | No | Required if camera type is default or 'PERSPECTIVE' |
|heightMode| Mode of camera height, 0: initial randomization, 1: procedual randomization | | Yes || -->
<!-- ## Function
|Function   |Description    |
|---    |---    |
|{attr_name}| Get attributes of trajectory.| -->
<h3><a class="header" href="#get-trajectory-and-its-attributes" id="get-trajectory-and-its-attributes">Get trajectory and its attributes</a></h3>
<p>Function explanation</p>
<ul>
<li><code>self.shader.world.trajectories</code>: Get trajectory list in the scene.</li>
<li><code>trajectory.{attr_name}</code>: Get attributes of trajectory.</li>
</ul>
<p>example:</p>
<pre><code class="language-python">from ksecs.ECS.processors.entity_processor import EntityProcessor
class ReadTrajDsl(EntityProcessor):
    def process(self):
        # loop all trajectories
        for traj in self.shader.world.trajectories:
            cameraHeight = traj.height
</code></pre>
<h2><a class="header" href="#add-trajectory" id="add-trajectory">Add trajectory</a></h2>
<h3><a class="header" href="#add-coverage-trajectory" id="add-coverage-trajectory">Add coverage trajectory</a></h3>
<ul>
<li><code>self.shader.world.add_trajectory({attr_name}={attr_value})</code>: create a new trajectory and add to the scene.</li>
</ul>
<p>example:</p>
<pre><code class="language-python">from ksecs.ECS.processors.entity_processor import EntityProcessor
class CreateTrajDsl(EntityProcessor):
  def process(self):
    for room in self.shader.world.rooms:

            # Get room center
            room_center_x, room_center_y = room.position

            # Create trajectory of initial camera
            camera = self.shader.world.create_camera(
                id=room.id,

                hfov=110,
                vfov=125,

                imageWidth=1280,
                imageHeight=720,

                position=[room_center_x, room_center_y, 70],

                fnumber=8,
                iso=100,
                shutterSpeed=2
            )

            self.shader.world.add_trajectory(
                id=room.id,

                initCamera=camera,

                fps=3,

                speed=1500,

                pitch=0,

                height=70,

                collisionPadding=350,

                boundary=room.boundary,

                type=&quot;COVERAGE&quot;
            )
</code></pre>
<h3><a class="header" href="#add-customized-trajectory" id="add-customized-trajectory">Add Customized trajectory</a></h3>
<pre><code class="language-python">from ksecs.ECS.processors.entity_processor import EntityProcessor
class CreateTrajDsl(EntityProcessor):
    def process(self):
        self.make_traj(**param)
</code></pre>
<!-- Explanation of **param: See attributes of Customized trajectory above. -->
<h2><a class="header" href="#example-2" id="example-2">Example</a></h2>
<p>The example for customized trajectory is shown in <a href="dsl/../examples/trajectory_sampling.html">SLAM</a> section.</p>
<!-- <span style="color:blue">*Comments:* Default values are missing in the following forms.</span>. --><h1><a class="header" href="#randomtrajectory" id="randomtrajectory">RandomTrajectory</a></h1>
<h2><a class="header" href="#attributes-9" id="attributes-9">Attributes</a></h2>
<table><thead><tr><th>Attribute</th><th>Type</th><th>Description</th><th>Default value</th><th>Required</th></tr></thead><tbody>
<tr><td>pitch</td><td>float</td><td>The angle of pitch</td><td>-</td><td>Yes</td></tr>
<tr><td>height</td><td>float</td><td>The height of camera. The unit is mm.</td><td>-</td><td>Yes</td></tr>
<tr><td>initCamera</td><td><code>Camera</code></td><td>Initialize camera. Input arguments are the same as <a href="dsl/dsl/camera.html">Camera</a>.</td><td>-</td><td>Yes</td></tr>
<tr><td>fps</td><td>int</td><td>Frames per second</td><td>-</td><td>Yes</td></tr>
<tr><td>speed</td><td>float</td><td>trajectory speed (the unit is mm/s)</td><td>-</td><td>Yes</td></tr>
<tr><td>colisionPadding</td><td>float</td><td>The radius of collision detection</td><td>-</td><td>Yes</td></tr>
<tr><td>time</td><td>float</td><td>duration of time (the unit is s)</td><td>-</td><td>Yes</td></tr>
</tbody></table>
<h1><a class="header" href="#coveragetrajectory" id="coveragetrajectory">CoverageTrajectory</a></h1>
<h2><a class="header" href="#attributes-10" id="attributes-10">Attributes</a></h2>
<table><thead><tr><th>Attribute</th><th>Type</th><th>Description</th><th>Default value</th><th>Required</th></tr></thead><tbody>
<tr><td>pitch</td><td>float</td><td>The angle of pitch</td><td>-</td><td>Yes</td></tr>
<tr><td>height</td><td>float</td><td>The height of camera. The unit is mm.</td><td>-</td><td>Yes</td></tr>
<tr><td>initCamera</td><td><code>Camera</code></td><td>Initialize camera. Input arguments are the same as <a href="dsl/dsl/camera.html">Camera</a>.</td><td>-</td><td>Yes</td></tr>
<tr><td>fps</td><td>int</td><td>Frames per second</td><td>-</td><td>Yes</td></tr>
<tr><td>speed</td><td>float</td><td>trajectory speed (the unit is mm/s)</td><td>-</td><td>Yes</td></tr>
<tr><td>colisionPadding</td><td>float</td><td>The radius of collision detection</td><td>-</td><td>Yes</td></tr>
<tr><td>boundary</td><td>list</td><td>Restriction range of trajectory</td><td>-</td><td>Yes</td></tr>
</tbody></table>
<h1><a class="header" href="#customizedtrajectory" id="customizedtrajectory">CustomizedTrajectory</a></h1>
<h2><a class="header" href="#attributes-11" id="attributes-11">Attributes</a></h2>
<!-- <span style="color:blue">*Comments:* This parameter list needs revision (e.g., description of keyPoints is incorrect).</span>.  -->
<table><thead><tr><th>Attribute</th><th>Type</th><th>Description</th><th>Required</th><th>Remark</th></tr></thead><tbody>
<tr><td>pitch</td><td>float</td><td>The angle of pitch</td><td>Yes</td><td></td></tr>
<tr><td>height</td><td>float</td><td>The height of camera. The unit is mm.</td><td>Yes</td><td></td></tr>
<tr><td>imageWidth</td><td>int</td><td>The width of rendered image</td><td>Yes</td><td></td></tr>
<tr><td>imageHeight</td><td>int</td><td>The height of rendered image</td><td>Yes</td><td></td></tr>
<tr><td>keyPoints</td><td>list</td><td>(Key points in image space. pixel indicies: [[[[x1, y1], [x2, y2], ...]]])</td><td>Yes</td><td>Three dimensional list. 1. list of keypoints set 2. list of pixel coordinates. 3. pixel coordinates</td></tr>
<tr><td>fps</td><td>int</td><td>Frames per second</td><td>-</td><td>Required if <code>frameCount</code> is not specified</td></tr>
<tr><td>speed</td><td>float</td><td>trajectory speed (the unit is mm/s)</td><td>-</td><td>Required if <code>frameCount</code> is not specified</td></tr>
<tr><td>speedMode</td><td>int</td><td>Mode for randomized speed, 0: initial randomization 1: procedual randomization</td><td>-</td><td>Required if <code>speed</code> is specified</td></tr>
<tr><td>frameCount</td><td>int</td><td>Total frame count</td><td>-</td><td>Required if <code>fps</code> is not specified</td></tr>
<tr><td>pitchMode</td><td>int</td><td>Mode of pitch randomization, 0: initial randomization, 1: procedual randomization</td><td>Yes</td><td></td></tr>
<tr><td>hfow</td><td>float</td><td>Horizontal field of view (the unit is degree)</td><td>-</td><td>Required if camera type is default or 'PERSPECTIVE'</td></tr>
<tr><td>vfow</td><td>float</td><td>Vertical field of view (the unit is degree)</td><td>-</td><td>Required if camera type is default or 'PERSPECTIVE'</td></tr>
<tr><td>orthoWidth</td><td>float</td><td>horizontal field of view (the unit is mm)</td><td>-</td><td>Required if camera type is default or 'PERSPECTIVE'</td></tr>
<tr><td>orthoHeight</td><td>float</td><td>vertical field of view (the unit is mm)</td><td>-</td><td>Required if camera type is default or 'PERSPECTIVE'</td></tr>
<tr><td>heightMode</td><td>int</td><td>Mode of camera height, 0: initial randomization, 1: procedual randomization</td><td>Yes</td><td></td></tr>
</tbody></table>
<h1><a class="header" href="#render-processor" id="render-processor">Render Processor</a></h1>
<p>In the Render Stage, MINERVAS system uses the generated scenes to generate 2D renderings with the photo-realistic rendering engine.</p>
<!-- ## Attributes
|Attributes |Type | Description    |
|---    |---    |--- |
| -|-|- | -->
<h2><a class="header" href="#function-3" id="function-3">Function</a></h2>
<table><thead><tr><th>Function</th><th>Description</th></tr></thead><tbody>
<tr><td>gen_rgb(distort=0)</td><td>Rendering photo-realistic image. distort(int) represent using distortion or not when rendering. See <a href="dsl/pixel_process/distortion.html">Distortion Simulation</a> for details.</td></tr>
</tbody></table>
<h3><a class="header" href="#example-rgb-rendering" id="example-rgb-rendering">Example: RGB rendering</a></h3>
<p>Output format:
3 channel * 8 bit</p>
<p>Usage:</p>
<pre><code class="language-python">from ksecs.ECS.processors.render_processor import RenderProcessor
class Render(RenderProcessor):
    def process(self, *args, **kwargs):
        self.gen_rgb(distort=0)
</code></pre>
<h1><a class="header" href="#pixel-processor" id="pixel-processor">Pixel Processor</a></h1>
<!-- ## Description -->
<p>In the pixel process stage, the system modifies imagery output pixel-wisely and generates the final dataset.
Users can implement a <code>PixelProcessor</code> to control the pixel process stage.</p>
<p>Specifically, users can </p>
<ol>
<li>select the output information with interest. (e.g., normal map, semantic map)</li>
<li>generate randomized image noise.</li>
<li>add distortion to rendered image.</li>
<li>visualize strucutre of room.</li>
</ol>
<!-- ## Attributes
|Attributes |Type | Description    |
|---    |---    |--- |
| -|-|- | -->
<h2><a class="header" href="#function-4" id="function-4">Function</a></h2>
<table><thead><tr><th>Function</th><th>Description</th></tr></thead><tbody>
<tr><td>gen_normal(distort=0)</td><td>Generate normal map. (distort: <code>int</code>)</td></tr>
<tr><td>gen_instance(distort=0)</td><td>Generate intance map. (distort: <code>int</code>)</td></tr>
<tr><td>gen_semantic(distort=0)</td><td>Generate semantic map. (distort: <code>int</code>)</td></tr>
<tr><td>gen_depth(distort=0, noise=0)</td><td>Generate depth map. (distort: <code>int</code>, noise: <code>int</code>)</td></tr>
<tr><td>gen_traj(**params)</td><td>Generate trajectory visualization (top-down view). (params: <code>dict</code>: parameter list from each type of <a href="dsl/../../trajectory.html">Trajectory</a>)</td></tr>
<tr><td>gen_albedo(distort=0)</td><td>Generate albedo map. (distort: <code>int</code>)</td></tr>
</tbody></table>
<!-- Each image is an instance of `class RenderResult`. -->
<!-- Based on these, users could modify image outputs. -->
<!-- More description about [RenderResult](./image.md) -->
<!-- 
## Example

```python
class PixelExample(PixelProcessor):
    def process(self, **kwargs):
        images = kwargs.get("images")
        albedo = images.get('rgb')
        depth = images.get('depth')
        normal = images.get('normal')
        ...
``` -->
<h1><a class="header" href="#output-selection" id="output-selection">Output selection</a></h1>
<p>In the MINERVAS system, there are several rendering output we support: </p>
<ul>
<li>RGB renderings </li>
<li>Normal map </li>
<li>Instance map </li>
<li>Semantic map</li>
<li>Depth map</li>
<li>Trajectory map</li>
</ul>
<!-- * Image based operation like noise  -->
<!-- * distortion, structure visualization are also supported. -->
<!-- What is the default output if PixelProcessor is not specified?  -->
<!-- <span style="color:blue">*Comments:* Argument list and description of each funtion should be added.</span> -->
<h2><a class="header" href="#function-list" id="function-list">Function list</a></h2>
<p><code>PixelProcessor</code> has several functions for selecting different output.</p>
<table><thead><tr><th>Function</th><th>Description</th></tr></thead><tbody>
<tr><td>gen_normal(distort=0)</td><td>Generate normal map. (distort: <code>int</code>)</td></tr>
<tr><td>gen_instance(distort=0)</td><td>Generate intance map. (distort: <code>int</code>)</td></tr>
<tr><td>gen_semantic(distort=0)</td><td>Generate semantic map. (distort: <code>int</code>)</td></tr>
<tr><td>gen_depth(distort=0, noise=0)</td><td>Generate depth map. (distort: <code>int</code>, noise: <code>int</code>)</td></tr>
<tr><td>gen_traj(**params)</td><td>Generate trajectory visualization (top-down view). (params: <code>dict</code>: parameter list from each type of <a href="dsl/pixel_process/dsl/../../trajectory.html">Trajectory</a>)</td></tr>
<tr><td>gen_albedo(distort=0)</td><td>Generate albedo map. (distort: <code>int</code>)</td></tr>
</tbody></table>
<!-- <span style="color:blue">*Comments:* Differences between `RenderProcessor` and `PixelProcessor`.</span> -->
<h2><a class="header" href="#examples" id="examples">Examples:</a></h2>
<h3><a class="header" href="#normal-map" id="normal-map">Normal map</a></h3>
<p>Output format:</p>
<ol>
<li>3 channel * 8 bit</li>
<li>Suppose r, g, b represent three color channels
<ul>
<li>[-1, 1] range of normal direction value are mapped to [0, 255]</li>
</ul>
</li>
</ol>
<p>Usage:</p>
<pre><code class="language-python">from ksecs.ECS.processors.pixel_processor import PixelProcessor
class NormalDsl(PixelProcessor):
    def process(self, **kwargs):
        self.gen_normal(distort=1)
</code></pre>
<h3><a class="header" href="#instance-map" id="instance-map">Instance map</a></h3>
<p>Output format:</p>
<ol>
<li>1 channel * 16 bit</li>
<li>Each pixel value represent a instance_id.
<ul>
<li>The mapping of pixel value and instance id can be found in <code>instance_map.json</code>.</li>
</ul>
</li>
</ol>
<p>Usage:</p>
<pre><code class="language-python">from ksecs.ECS.processors.pixel_processor import PixelProcessor
class InstanceDsl(PixelProcessor):
    def process(self, **kwargs):
        self.gen_instance(distort=0)
</code></pre>
<!-- Notes: -->
<!-- <span style="color:blue">*Comments:* TODO.</span> -->
<!-- 1. normal_threshold: -->
<!-- 2. merge_bias -->
<h3><a class="header" href="#semantic-map" id="semantic-map">Semantic map</a></h3>
<p>Output format:</p>
<ol>
<li>1 channel * 16 bit</li>
<li>Each pixel value represents a label_id</li>
</ol>
<p>Usage:</p>
<pre><code class="language-python">from ksecs.ECS.processors.pixel_processor import PixelProcessor
class SemanticDsl(PixelProcessor):
    def process(self, **kwargs):
        self.gen_semantic(distort=0)
</code></pre>
<!-- Notes: -->
<!-- <span style="color:blue">*Comments:* TODO.</span> -->
<!-- 1. normal_threshold: -->
<h3><a class="header" href="#depth-map" id="depth-map">Depth map</a></h3>
<p>Output format:</p>
<ol>
<li>1 channel * 16 bit</li>
</ol>
<p>Usage:</p>
<pre><code class="language-python">from ksecs.ECS.processors.pixel_processor import PixelProcessor
class DepthDsl(PixelProcessor):
    def process(self, **kwargs):
        self.gen_depth(distort=0, noise=1)
</code></pre>
<h3><a class="header" href="#trajectory-map" id="trajectory-map">Trajectory map</a></h3>
<p>Visualize trajectory in the top-down view rendering result. Customized trajectory and bow-shape trajectory are currently supported.</p>
<p>Usage:</p>
<pre><code class="language-python">from ksecs.ECS.processors.render_processor import RenderProcessor
class TrajDSL(RenderProcessor):
    def process(self, *args, **kwargs):
        self.gen_traj(**params)
</code></pre>
<p>Notes:
**params to be specified are in <a href="dsl/pixel_process/./dsl/../../trajectory.html">Trajectory</a>.</p>
<!-- 1. type: int. Select trajectory type.
    * 0: (DEFINED) Customized trajectory
    * 1: (COVERAGE) Bow-shape trajectory
    * 2: (RANDOM) Random trajectory -->
<!-- <span style="color:blue">*Comments:* More parameters?.</span> -->
<h3><a class="header" href="#albedo-map" id="albedo-map">Albedo map</a></h3>
<p>Ouput format:
4 channel * 8 bits</p>
<pre><code class="language-python">from ksecs.ECS.processors.pixel_processor import PixelProcessor
class AlbedoDsl(PixelProcessor):
    def process(self, **kwargs):
        self.gen_albedo(distort=0)
</code></pre>
<h1><a class="header" href="#noise-simulation" id="noise-simulation">Noise simulation</a></h1>
<p>MINERVAS system supports to simulate four common image noises:</p>
<ol>
<li>Gaussian Noise</li>
<li>Poisson Noise</li>
<li>Salt and Pepper Noise</li>
<li>Kinect Noise</li>
</ol>
<!-- ## Instructions for use -->
<!-- ### Description -->
<h2><a class="header" href="#function-5" id="function-5">Function</a></h2>
<table><thead><tr><th>Function</th><th>Description</th></tr></thead><tbody>
<tr><td>add_depth_noise(img, noise_type)</td><td>img (<code>numpy.ndarray</code> with <code>dtype=uint16</code>) is the image to be processed. <code>noise_type</code> is an integer flag for noise type. See following list.</td></tr>
</tbody></table>
<!-- Relationship of noise type flag with noise model: -->
<pre><code>0: GaussianNoiseModel
1: PoissonNoiseModel
2: SaltAndPepperNoiseModel
3: KinectNoiseModel
</code></pre>
<!-- `add_depth_noise(img, noise_type)` can be called according to the above use cases -->
<!-- among them -->
<!-- ### Example
Following DSL shows the procedure of adding the noise to an exising image.
```python
class TestPixelDsl(PixelProcessor):
     def process(self, **kwargs):
         for cid, img in self.shader.image_handler.load_images("camera_depth.png", mode="pillow"):
             img_after_noise = self.shader.image_handler.add_depth_noise(img, 3)
             self.shader.image_handler.save_files(
                 cid, content=img_after_noise, suffix="png", name='depth'
             )
``` --><h1><a class="header" href="#distortion-simulation" id="distortion-simulation">Distortion Simulation</a></h1>
<p>MINERVAS system supports distortion simulation.</p>
<p>Following function has the parameter for distortion.</p>
<!-- ``` -->
<ul>
<li>get_rgb(distort=0, noise=0)</li>
<li>gen_normal(distort=0, noise=0)</li>
<li>gen_instance(distort=0, noise=0)</li>
<li>gen_semantic(distort=0, noise=0)</li>
<li>gen_depth(distort=0, noise=0)</li>
<li>gen_albedo(distort=0, noise=0)</li>
</ul>
<!-- ``` -->
<!-- Notes: -->
<!-- flag is a integer flag. -->
<p>Parameter <code>distort</code> is an integer flag:</p>
<ul>
<li>0: No distortion</li>
<li>1: Add distortion</li>
</ul>
<h3><a class="header" href="#example-3" id="example-3">Example</a></h3>
<p>Add distortion when rendering photo-realistic image.</p>
<pre><code class="language-python">class AddDistortionDsl(PixelProcessor):
    def process(self, **kwargs):
        gen_rgb(distort=1, noise=0)
</code></pre>
<h1><a class="header" href="#examples-1" id="examples-1">Examples</a></h1>
<p>In this section, we will show serveral examples associated with corresponding DSL code, which includes:</p>
<ul>
<li><a href="examples/layout_estimation.html">Layout Estimation</a></li>
<li><a href="examples/semantic_segmentation.html">Semantic Segmentation</a></li>
<li><a href="examples/depth_estimation.html">Depth Estimation</a></li>
<li><a href="examples/trajectory_sampling.html">SLAM</a></li>
</ul>
<!-- - [Sweeping Robot](dsl/dsl_sample_code.md) -->
<h1><a class="header" href="#layout-estimation" id="layout-estimation">Layout Estimation</a></h1>
<h2><a class="header" href="#introduction" id="introduction">Introduction</a></h2>
<p>Layout estimiation is a challenging task in 3D vision. Several datasets are proposed to tackle this problem, including the large scale synthetic dataset Structured3D. 
MINERVAS system has the ability to generate dataset like Structured3D.</p>
<p>Here we show the DSL for generating data which helps boosting the current layout estimation task.</p>
<!-- We will show the ability of Minvervas by agumentating the layout estimation task, which obtains state-of-the-art results. -->
<p>More details can be found in the <a href="https://arxiv.org/pdf/2107.06149.pdf">paper</a> and <a href="https://drive.google.com/file/d/1avGTr44sGrWx_jWiNYEIrp3R7jbNPOgj/view">supplementary document</a>.</p>
<h2><a class="header" href="#dsl-code" id="dsl-code">DSL code</a></h2>
<!-- For room layout estimation task, we create a
filter rule using DSL in the Scene Process Stage, and setting
the type of camera as “panorama”. We also use the sampler
of the transform component to randomly move cameras, and
use the output component to write out corner positions and
camera parameters. -->
<!-- 
First, we import some necessary packages.
```python
from ksecs.ECS.processors.scene_processor import SceneProcessor
from ksecs.ECS.processors.entity_processor import EntityProcessor
from ksecs.ECS.processors.pixel_processor import PixelProcessor
from ksecs.ECS.processors.structure_processor import StructureProcessor
import copy
import glm
``` -->
<!-- First, we filter scenes which get rooms with manhattan layout assumption in the Scene Process Stage, as implemented in the `ManhattanSceneFilter`.

In the entity process stage, we first filter cameras in the relative empty rooms as shown in `CameraFilter`.

Then, we setup the camera parameters in `CameraSetting`. We also adopt domain randomization for cameras as shown in `CameraRandomizer`.

After the rendering process stage, we setup customized output for generating information about room structure as shown in `StructureOutput`. -->
<p>In this example, we generate both panoramic rgb images and scene structure information.</p>
<p>We first filter scenes with Manhattan-world assumption using <code>ManhattanSceneFilter</code> in the Scene Process Stage. </p>
<p>In the Entity Process Stage, we then remove the cameras in the relatively empty rooms using <code>CameraFilter</code>. Next, we set the camera parameters (\eg, camera type and image resolution) using <code>CameraSetting</code>. 
We randomize the positions of the cameras using <code>CameraRandomizer</code>.</p>
<p>After the Rendering Process Stage, we record the positions of room corners and cameras using <code>StructureOutput</code>.</p>
<!-- For this task, we set the type of camera to `PANORAMA` in Entity Process Stage, and output the corner and camera parameters in the Entity Process Stage. -->
<pre><code class="language-python">from ksecs.ECS.processors.scene_processor import SceneProcessor
import glm
import copy
import sys
class ManhattanSceneFilter(SceneProcessor):
    def is_manhattan_scene(self):
        # Check Manhattan assumption for each room
        Valid = False
        for room in self.shader.world.rooms:
            corners = room.boundary
            shift_corners = copy.deepcopy(corners)
            shift_corners.append(shift_corners.pop(0))
            for i in range(len(shift_corners)):
                direction = glm.vec2(shift_corners[i]).xy - glm.vec2(corners[i]).xy
                direction = glm.normalize(direction)
                shift_corners[i] = direction
            EPSILON = 0.001
            MANHATTAN = True
            for i in range(len(shift_corners)):
                cos = glm.dot(shift_corners[i], shift_corners[(i + 1) % len(shift_corners)])
                if cos &gt; EPSILON:
                    MANHATTAN = False
                    break
            if MANHATTAN:
                Valid = True
        return Valid

    def process(self):
        if not self.is_manhattan_scene():
            sys.exit(7)

from ksecs.ECS.processors.entity_processor import EntityProcessor
from shapely.geometry import Point
class CameraFilter(EntityProcessor):
    def is_valid_room(self, room, num_furniture=3):
        # Check if the number of furniture in the room is above threshold.
        polygon = room.gen_polygon()
        count = 0
        for ins in self.shader.world.instances:
            if not ins.type == 'ASSET':
                continue
            if polygon.contains(Point([ins.transform[i] for i in [3, 7, 11]])):
                count += 1
        return count &gt; num_furniture
    
    def delete_cameras_in_room(self, room):
        polygon = room.gen_polygon()
        for camera in self.shader.world.cameras:
            if polygon.contains(Point([camera.position[axis] for axis in &quot;xyz&quot;])):
                self.shader.world.delete_entity(camera)

    def process(self):
        # We only use rooms with more than 4 assets
        for room in self.shader.world.rooms:
            if not self.is_valid_room(room, 4):
                self.delete_cameras_in_room(room)

class CameraSetting(EntityProcessor):
    def process(self):
        for camera in self.shader.world.cameras:
            camera.set_attr(&quot;imageWidth&quot;, 1024)
            camera.set_attr(&quot;imageHeight&quot;, 512)
            camera.set_attr(&quot;cameraType&quot;, &quot;PANORAMA&quot;)

import numpy as np
class CameraRandomizer(EntityProcessor):
    def process(self):
        for camera in self.shader.world.cameras:
            random_vec = np.random.normal(0, 1, size=3)
            camera_pos = np.array([camera.position[axis] for axis in &quot;xyz&quot;])
            randomized_pos = camera_pos + random_vec * np.array([500.0, 500.0, 50.0])
            camera.set_attr('position', x=randomized_pos[0], y=randomized_pos[1], z=randomized_pos[2])
            camera.set_attr('lookAt', z=randomized_pos[2])

from ksecs.ECS.processors.render_processor import RenderProcessor
class Render(RenderProcessor):
    def process(self, *args, **kwargs):
        self.gen_rgb(distort=0, noise=0)

from ksecs.ECS.processors.structure_processor import StructureProcessor
class StructureOutput(StructureProcessor):
    def process(self):
        # write out the corners of the rooms in the scene
        for room in self.shader.world.rooms:
            for plane, height in zip([&quot;floor&quot;, &quot;ceiling&quot;], [0, self.shader.world.levels[0].height]):
                corners = []
                for corner in room.boundary:
                    corners.append({'x': corner[0], 'y': corner[1], 'z': height})
                self.shader.world.pick(
                    corners=corners,
                    catName=plane,
                    type='corners',
                    id=f&quot;{room.roomId}_{plane}&quot;
                )
        # write out cameras in the scene
        for camera in self.shader.world.cameras:
            self.shader.world.pick(
                type=&quot;camera&quot;,
                position=camera.position,
                id=camera.id
            )
</code></pre>
<h2><a class="header" href="#minervas-output-samples" id="minervas-output-samples">MINERVAS output samples</a></h2>
<!-- TBD. -->
<p><img src="examples/./../examples_figs/layout_samples.png" alt="layout_samples" /></p>
<!-- ## Experimental Setup

In this experiment, we use MatterportLayout [[1, 2]](#1) as the real data. The dataset consists of 1,647 images for training, 190 images for validation, and 458 images for testing. Then, we synthesize 120K panorama images from 80K scenes using our system. Each panorama image corresponds to one room in scenes.
Following [[2]](#2), we adopt four standard metrics: 3D IoU,
2D IoU, RMSE and the accuracy under the threshold (δ1).
We adopt HorizonNet [[3]](#3) as the baseline approach. We use
an Adam optimizer with an initial learning rate of 3 × 10−4
with a polynomial decay policy. We set the mini-batch size
to 24. We also use two training strategies in this experi-
ment, i.e., “r” and “s + r”. In “s + r”, each batch contains
16 images from the real dataset and 8 from the synthetic
dataset. For each strategy, we train the whole network for
30K iterations

## Experiment Results

Results are reported in Table 1. As can be seen, the model trained on both the synthetic and real datasets achieves the best result. After augmenting the synthetic data, the network can predict the corner's position more accurately. Meanwhile, the predicted number of corners is more accurate. It demonstrates that our synthetic data could be used to improve the performance of the network. Qualitative results are shown in Figure 2.

![fig_layout](./../examples_figs/fig_layout.png)
![table_layout](./../examples_figs/table_layout.png) -->
<!-- 
## References
<a id="1">[1]</a> 
Angel X. Chang, Angela Dai, Thomas A. Funkhouser, Maciej Halber, Matthias Nießner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from RGB-D data in indoor environments. In 3DV, pages 667–676, 2017.

<a id="2">[2]</a> 
Chuhang Zou, Jheng-Wei Su, Chi-Han Peng, Alex Colburn, Qi Shan, Peter Wonka, Hung-Kuo Chu, and Derek Hoiem. Manhattan room layout reconstruction from a single 360 image: A comparative study of state-of-the-art methods. International Journal of Computer Vision, 2021.

<a id="3">[3]</a> 
Cheng Sun, Chi-Wei Hsiao, Min Sun, and Hwann-Tzong Chen. Horizonnet: Learning room layout with 1d representation and pano stretch data augmentation. In CVPR, pages 1047–1056, 2019. --><h1><a class="header" href="#semantic-segmentation" id="semantic-segmentation">Semantic Segmentation</a></h1>
<h2><a class="header" href="#introduction-1" id="introduction-1">Introduction</a></h2>
<p>Depth estimiation is an essential task in indoor scene understanding. We show the performance gain by generating dataset using our system.</p>
<p>Here we show the DSL for generating semantic mask in NYUd-v2-40 format which helps boosting the current semantic segmentation task.</p>
<p>More details can be found in the <a href="https://arxiv.org/pdf/2107.06149.pdf">paper</a> and <a href="https://drive.google.com/file/d/1avGTr44sGrWx_jWiNYEIrp3R7jbNPOgj/view">supplementary document</a>.</p>
<h2><a class="header" href="#dsl-code-1" id="dsl-code-1">DSL code</a></h2>
<!-- For room layout estimation task, we create a
filter rule using DSL in the Scene Process Stage, and setting
the type of camera as “panorama”. We also use the sampler
of the transform component to randomly move cameras, and
use the output component to write out corner positions and
camera parameters. -->
<p>In this example, we generate semantic labels and show the domain randomization ability of the MINERVAS system.</p>
<p>First, in the Entity Process Stage we filter out cameras in the rooms which do not has more than 4 furniture using <code>CameraFilter</code>. 
For each camera, we set the camera model as <code>panorama</code> and the resolution of the image to 640 * 480 in class <code>CameraSetting</code>. </p>
<p>We then utilize the system's ability for domain randomization to generate data. Specifically, we randomize room layout in the Scene Process Stage using <code>FurnitureLayoutSampler</code>. Then, we randomize material, model and camera in the Entity Process Stage using <code>EntityRandomizer</code>. </p>
<p>In the Pixel Process Stage, we generate semantic label with NYUv2 40 label set as shown in <code>SemanticOutput</code> class.</p>
<pre><code class="language-python">from ksecs.ECS.processors.entity_processor import EntityProcessor
from shapely.geometry import Point
class CameraFilter(EntityProcessor):
    def is_valid_room(self, room, num_furniture=3):
        # Check if the number of furniture in the room is above threshold.
        polygon = room.gen_polygon()
        count = 0
        for ins in self.shader.world.instances:
            if not ins.type == 'ASSET':
                continue
            if polygon.contains(Point([ins.transform[i] for i in [3, 7, 11]])):
                count += 1
        return count &gt; num_furniture
    
    def delete_cameras_in_room(self, room):
        polygon = room.gen_polygon()
        for camera in self.shader.world.cameras:
            if polygon.contains(Point([camera.position[axis] for axis in &quot;xyz&quot;])):
                self.shader.world.delete_entity(camera)

    def process(self):
        # We only use rooms with more than 4 assets
        for room in self.shader.world.rooms:
            if not self.is_valid_room(room, 4):
                self.delete_cameras_in_room(room)

class CameraSetting(EntityProcessor):
    def process(self):
        for camera in self.shader.world.cameras:
            camera.set_attr(&quot;imageWidth&quot;, 1024)
            camera.set_attr(&quot;imageHeight&quot;, 512)
            camera.set_attr(&quot;cameraType&quot;, &quot;PANORAMA&quot;)

# Randomize layout
from ksecs.ECS.processors.scene_processor import SceneProcessor
class FurnitureLayoutSampler(SceneProcessor):
    def process(self):
        for room in self.shader.world.rooms:
            room.randomize_layout(self.shader.world)

import numpy as np
class EntityRandomizer(EntityProcessor):
    def randomize_model_material(self):
        for instance in self.shader.world.instances:
            # Randomize material
            self.shader.world.replace_material(
                id=instance.id,
                type='REPLACE_ALL'
            )
            if instance.type == 'ASSET':
                # Randomize model
                self.shader.world.replace_model(
                    id=instance.id
                )

    def randomize_light(self):
        for light in self.shader.world.lights:
            # Randonly adjust the color temperature
            light._tune_temp(1)
            # Randomly adjust the light intensity
            light.tune_random(1.2)

    def randomize_camera(self):
        for camera in self.shader.world.cameras:
            random_vec = np.random.normal(0, 1, size=3)
            camera_pos = np.array([camera.position[axis] for axis in &quot;xyz&quot;])
            randomized_pos = camera_pos + random_vec * np.array([500.0, 500.0, 100.0])
            camera.set_attr('position', x=randomized_pos[0], y=randomized_pos[1], z=randomized_pos[2])
            camera.set_attr('lookAt', z=randomized_pos[2])

    def process(self):
        self.randomize_model_material()
        self.randomize_light()
        self.randomize_camera()

from ksecs.ECS.processors.render_processor import RenderProcessor
class Render(RenderProcessor):
    def process(self, *args, **kwargs):
        self.gen_rgb(distort=0, noise=0)

from ksecs.ECS.processors.pixel_processor import PixelProcessor
from ksecs.resources import NYU40_MAPPING
class SemanticOutput(PixelProcessor):
    def process(self, **kwargs):
        self.gen_semantic(label_arch=NYU40_MAPPING)
</code></pre>
<!-- First, we import some necessary packages.
```python
from ksecs.ECS.processors.pixel_processor import PixelProcessor
from ksecs.ECS.processors.entity_processor import EntityProcessor
```

For the semantic segmentation task, we set the type of camera to panorama in the Entity Process Stage, and output the semantic label image in the Pixel Process Stage.
We also map our semantic label to the NYUv2 40 label set using our DSL.

```python
class CameraSetting(EntityProcessor):
    def process(self):
        for camera in self.shader.world.cameras:
            camera.set_attr("imageWidth", 1024)
            camera.set_attr("imageHeight", 512)
            camera.set_attr("cameraType", "PANORAMA")
```

```python
class LabelMapping(PixelProcessor):
    def process(self, **kwargs):
        self.gen_semantic(label_arch=NYU40_Mapping)
``` -->
<h2><a class="header" href="#minervas-output-samples-1" id="minervas-output-samples-1">MINERVAS output samples</a></h2>
<!-- TBD. -->
<!-- ![semantic_samples](./../examples_figs/semantic_samples.png) -->
<p><img src="examples/./../examples_figs/semantic_samples_nyu40.png" alt="semantic_samples" /></p>
<!-- ## Experimental Setup

In this experiment, we use 2D-3D-S [[1]](#1) as the real data. We split the images into 955 for
training, 84 for validation, and 373 for testing. Then, we
synthesize 12k panoramic images using our system. Each
panorama image corresponds to one room in scenes.
We use an SGD optimizer with an initial learning rate of
2 × 10−2 with a polynomial decay policy, momentum 0.9,
and weight decay of 10−4. We set the mini-batch size to
8. In “s + r”, each batch contains 4 images from the real
dataset and 4 from the synthetic dataset. For each strategy,
we train the whole network for 10k iterations.

## Results

We show more qualitative results of semantic segmentation in Figure 5. As can be seen, training
on the synthetic and real dataset achieves the best result.
The boundary is more clear in semantic segmentation. It
demonstrates that our synthetic data could be used to im-
prove the performance of network.

![fig_segmentation](./../examples_figs/fig_semantic.png)
<!-- ![table_layout](./../examples_figs/table_layout.png) -->
<!-- ## References
<a id="1">[1]</a> 
Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese. Joint 2d-3d-semantic data for indoor scene understanding. arXiv preprint arXiv:1702.01105, 2017. --><h1><a class="header" href="#depth-estimation" id="depth-estimation">Depth Estimation</a></h1>
<h2><a class="header" href="#introduction-2" id="introduction-2">Introduction</a></h2>
<p>Depth estimiation is an essential task in indoor scene understanding. We show the performance gain by generating dataset using our system.</p>
<p>Here we show the DSL for generating depth images which helps boosting the current depth estimation task.
More details can be found in the <a href="https://arxiv.org/pdf/2107.06149.pdf">paper</a> and <a href="https://drive.google.com/file/d/1avGTr44sGrWx_jWiNYEIrp3R7jbNPOgj/view">supplementary document</a>.</p>
<h2><a class="header" href="#dsl-code-2" id="dsl-code-2">DSL code</a></h2>
<!-- For room layout estimation task, we create a
filter rule using DSL in the Scene Process Stage, and setting
the type of camera as “panorama”. We also use the sampler
of the transform component to randomly move cameras, and
use the output component to write out corner positions and
camera parameters. -->
<p>In this example, we generate cameras by rules and generate depth images.</p>
<p>In the Entity Process Stage, we manually setup cameras in the valid rooms as shown in class <code>CameraSetter</code>. Specifially, we set the position by a rule and set the resolution of the image to 640* 480 and horizontal field-of-view (FoV) to 57 to align with Microsoft Kinect used in the NYUv2 dataset. </p>
<p>In the Pixel Process Stage, we setup the depth output in class <code>DepthOutput</code>.</p>
<!-- TBD. -->
<pre><code class="language-python">from ksecs.ECS.processors.entity_processor import EntityProcessor
from shapely.ops import nearest_points
from shapely.geometry import Point
import numpy as np
import math
 
class CameraSetter(EntityProcessor):
    # Set a camera in the room by a heuristic rule
    def set_camera(self, room):
        width = 640
        height = 480
        # look from right corner of the room
        camera_height = min(2000, self.shader.world.levels[0].height)
        right_corner = np.max(room.boundary, axis=0)
        room_polygon = room.gen_polygon()
        border, point = nearest_points(room_polygon, Point(right_corner))
        pos = [border.x - 200, border.y - 200, camera_height - 200]
        # look at room center
        look_at = room.position + [camera_height / 2]
            
        hfov = 57
        tan = (math.tan(math.radians(hfov / 2))) / (width / height)
        vfov = math.degrees(math.atan(tan)) * 2
            
        self.shader.world.add_camera(
            id=f&quot;view_{room.roomId}&quot;,
            cameraType=&quot;PERSPECTIVE&quot;,
            hfov=hfov,
            vfov=vfov,
            imageWidth=width,
            imageHeight=height,
            position=pos,
            lookAt=look_at,
            up=[0, 0, 1]
        )
    
    def is_valid_room(self, room, num_furniture=3):
        # Check if the number of furniture in the room is above threshold.
        polygon = room.gen_polygon()
        count = 0
        for ins in self.shader.world.instances:
            if not ins.type == 'ASSET':
                continue
            if polygon.contains(Point([ins.transform[i] for i in [3, 7, 11]])):
                count += 1
        return count &gt; num_furniture

    def process(self):
        # Delete all existing cameras
        for camera in self.shader.world.cameras:
            self.shader.world.delete_entity(camera)
 
        # Set new camera for valid room
        for room in self.shader.world.rooms:
            if self.is_valid_room(room):
                self.set_camera(room)

from ksecs.ECS.processors.render_processor import RenderProcessor
class Render(RenderProcessor):
    def process(self, *args, **kwargs):
        self.gen_rgb(distort=0, noise=0)

from ksecs.ECS.processors.pixel_processor import PixelProcessor
class DepthOutput(PixelProcessor):
    def process(self, **kwargs):
        self.gen_depth(distort=0, noise=0)
</code></pre>
<!-- 
First, we import some necessary packages.
```python
from ksecs.ECS.processors.entity_processor import EntityProcessor
```

We filter scenes that contain rooms for manhattan layout estimation in the Scene Process Stage.

For the depth estimation task, we set the resolution of the image to 640x480 and horizontal field-of-view (FoV) to 53 degree in the Entity Process Stage, which is the same as those in Microsoft Kinect used in the NYUv2 dataset.

```python
class CameraSetting(EntityProcessor):
    def process(self):
        for camera in self.shader.world.cameras:
            camera.set_attr("imageWidth", 640)
            camera.set_attr("imageHeight", 480)
            camera.set_attr("cameraType", "PERSPECTIVE")
            camera.set_attr("hfov", 53)
``` -->
<h2><a class="header" href="#minervas-output-samples-2" id="minervas-output-samples-2">MINERVAS output samples</a></h2>
<!-- TBD. -->
<p><img src="examples/../examples_figs/depth_samples.png" alt="depth_samples" /></p>
<!-- ## Experimental Setup

In this experiment, we use NYUv2 as 
the real data. We split the images into 795 for training and
654 for testing. Then, we synthesize 14k images using our
system.
We use an SGD optimizer with an initial learning rate
1 × 10−4 with polynomial decay policy, momentum 0.9,
and weight decay 5 × 10−4. We set the mini-batch size to 8. In “s + r”, each batch contains 4 images from the real
dataset and 4 from the synthetic dataset. For each strategy,
we train the whole network for 15k iterations. -->
<!-- ## Results. 
We show more qualitative results in
Figure 7. As one can see, training on the synthetic and real
dataset, the network generates more accurate estimations
than that only using real images for training. The noise has
been significantly reduced in the depth estimation.

![fig_layout](./../examples_figs/fig_depth.png) --><h1><a class="header" href="#slam" id="slam">SLAM</a></h1>
<h2><a class="header" href="#introduction-3" id="introduction-3">Introduction</a></h2>
<p>Simultaneous localization and mapping (SLAM) is the computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it<a href="examples/trajectory_sampling.html#2"><sup>1</sup></a>.
Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed reality and robotic applications like SLAM. 
In dense SLAM, a space is mapped by fusing the data from a
moving sensor into a representation of the continuous surfaces. </p>
<p>In this experiment, we utilize our system for the
SLAM learning task by generating images from a sequence
of camera views, generated by our trajectory sampler.</p>
<p>More results of SLAM application can be found in the <a href="https://drive.google.com/file/d/1avGTr44sGrWx_jWiNYEIrp3R7jbNPOgj/view">supplementary document</a>.</p>
<h2><a class="header" href="#dsl-code-3" id="dsl-code-3">DSL code</a></h2>
<p>Our system give users the ability to create their custom camera trajectory.</p>
<!-- The ECS-D architecture and basic sampler give users the ability to create their scene sample strategy.  -->
<p>We show an example of custom trajectory sampler DSL below. </p>
<pre><code class="language-python">import numpy as np
from math import sqrt
from shapely.geometry import Point
from shapely.ops import nearest_points
import glm

from ksecs.ECS.processors.entity_processor import EntityProcessor
from ksecs.ECS.processors.render_processor import RenderProcessor


class CustomTrajectorySampler(EntityProcessor):

    def calculate_vel(self, velocity, step_time):
        mu = glm.normalize(glm.vec2(np.random.normal(0, 1, size=2)))
        FORCE = 100
        f = FORCE * mu
        PHI, A, C_D, M = 1.204, 0.09, 0.1, 1.0
        d = -0.5 * glm.normalize(velocity) * PHI * A * C_D * glm.dot(velocity, velocity)
        velocity = velocity + (d + f) * M * step_time
        S_MAX = 10
        if S_MAX &lt; sqrt(glm.dot(velocity, velocity)):
            velocity = S_MAX * glm.normalize(velocity)
        return velocity

    def process(self):
        for camera in self.shader.world.cameras:
            self.shader.world.delete_entity(camera)

        key_points = []
        for room in self.shader.world.rooms:
            # init
            room_points = []
            room_polygon = room.gen_polygon()
            camera_vel = glm.normalize(glm.vec2(np.random.normal(0, 1, size=2)))
            camera_pos = glm.vec2(room.position)
            # calcaulate key points
            length_of_trajectory, delta_time, scale = 5, 0.03, 1000
            for i in range(length_of_trajectory):
                # update camera params
                camera_vel = self.calculate_vel(camera_vel, delta_time)
                new_position = camera_pos + scale * camera_vel * delta_time
                next_camera_point = Point(tuple(new_position.xy))
                if not next_camera_point.within(room_polygon):
                    p1, p2 = nearest_points(room_polygon, next_camera_point)
                    normal = glm.normalize(glm.vec2(p1.x - p2.x, p1.y - p2.y))
                    camera_vel = glm.reflect(camera_vel, normal)
                camera_pos = camera_pos + scale * camera_vel * delta_time
                room_points.append(list(camera_pos))
            key_points.append(room_points)

        self.make_traj(
            imageHeight=960,
            imageWidth=1280,
            keyPoints=key_points,
            speed=1200,
            fps=3,
            speedMode=1,
            pitchMode=1,
            pitch=[-10, 10],
            hfov=70,
            vfov=55,
            height=1400,
            heightMode=1,
            cameraType=&quot;PERSPECTIVE&quot;
        )

class Render(RenderProcessor):
    def process(self, *args, **kwargs):
        self.gen_rgb(distort=0, noise=0)
</code></pre>
<h2><a class="header" href="#minervas-output-samples-3" id="minervas-output-samples-3">MINERVAS output samples</a></h2>
<p>After running with the DSL above, we can get a sequence of images (color and depth) along camera trajectory as shown below.</p>
<p><img src="examples/./../examples_figs/trajectory.png" alt="trajectory_output" /></p>
<!-- ## Experimental Setup -->
<!-- ## SLAM Result -->
<!-- We evaluated the usability of our generated data based on the open source SLAM algorithm Bundlefusion [[2]](#2) -->
<!-- A qualitative result is shown below. Scenes can  -->
<!-- be robustly reconstructed, as well as the textures of objects.  -->
<!-- This demonstrates that our pipeline can output the proposed  -->
<!-- dataset for the SLAM algorithm, and that the generated trajectory provides a good view of the interior.  -->
<!-- The results of our experiments indicate that our synthetic scenes with configurable attributes and background can be utilized to diagnose the SLAM algorithm. -->
<!-- ![trajectory_output](./../examples_figs/fig_3d_reconstruction.png) -->
<h2><a class="header" href="#references" id="references">References</a></h2>
<p><a id="1">[1]</a> 
wikipedia. Simultaneous localization and mapping. <a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping">https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping</a>.</p>
<!-- <a id="2">[2]</a> 
Angela Dai, Matthias Nießner, Michael Zollhöfer, Shahram Izadi, and Christian Theobalt. Bundlefusion: Real-time globally consistent 3d reconstruction using on-the-fly surface reintegration. ACM Transactions on Graphics (TOG), 36(4):1, 2017. -->
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
